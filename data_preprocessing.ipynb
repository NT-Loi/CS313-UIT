{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb0b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load list of paper IDs\n",
    "data_dir = \"raw_data\"\n",
    "with open(data_dir + \"/processed.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    paper_ids = data[\"arxiv_id\"]\n",
    "\n",
    "records = []\n",
    "\n",
    "# Iterate through each paper JSON file\n",
    "for pid in paper_ids:\n",
    "    file_path = Path(data_dir + f\"/{pid}.json\")\n",
    "    if not file_path.exists():\n",
    "        continue\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        paper_data = json.load(f)\n",
    "\n",
    "        # citations_by_year stays as a dict directly\n",
    "        record = {**paper_data}\n",
    "        records.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88298e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_df = pd.json_normalize(df['venue'],\n",
    "                             sep='.')\n",
    "df = pd.concat([df, venue_df], axis=1)\n",
    "df.drop(columns=['venue'], inplace=True)\n",
    "df.rename(columns={'name': 'venue_name',\n",
    "                   'type': 'venue_type',\n",
    "                   'ranking': 'venue_ranking'},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1034da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['pdf_url', 'embedding', 'venue_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88dd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_category(cat):\n",
    "    match = re.search(r'\\((.*?)\\)', cat)\n",
    "    return match.group(1) if match else cat.strip()\n",
    "\n",
    "df['categories'] = df['categories'].apply(lambda lst: [normalize_category(c) for c in lst])\n",
    "df['primary_category'] = df['primary_category'].apply(normalize_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c5205",
   "metadata": {},
   "source": [
    "### Remove paper with num_pages = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2343f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['num_pages'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb592de",
   "metadata": {},
   "source": [
    "### Fill missing values in github_stars = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a715b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['github_stars'].isna(), 'github_stars'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65c6a8",
   "metadata": {},
   "source": [
    "### Fill missing (venue.type, venue.ranking) = (preprint, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['venue_type'].isna(), 'venue_type'] = 'preprint'\n",
    "df.loc[df['venue_ranking'].isna(), 'venue_ranking'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a1abc",
   "metadata": {},
   "source": [
    "### Add columns 'citations_after_years{0: ..., 1: ..., 2: ..., 3: ...}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e54846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "df['published_year'] = df['published_date'].dt.year\n",
    "\n",
    "# Iterate over each row to calculate citations after X years\n",
    "def calculate_citations_after_years(row):\n",
    "    citations_by_year = row['citations_by_year']\n",
    "    published_year = row['published_year']\n",
    "    result = {}\n",
    "    for year in range(published_year, 2024):\n",
    "        result[year-published_year] = 0\n",
    "    for year, count in citations_by_year.items():\n",
    "        year = int(year)\n",
    "        result[year - published_year] = count\n",
    "    return result\n",
    "\n",
    "def reset_published_year(row):\n",
    "    citations = row.get(\"citations_by_year\", {})\n",
    "    if not citations:\n",
    "        return row[\"published_date\"].year\n",
    "    \n",
    "    cited_years = [int(y) for y, c in citations.items()]\n",
    "    return min(cited_years)  # first year with citations\n",
    "    \n",
    "df[\"published_year\"] = df.apply(reset_published_year, axis=1)\n",
    "df['citations_after_years'] = df.apply(calculate_citations_after_years, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb42a23",
   "metadata": {},
   "source": [
    "### Assign outliers' venue ranking by their nearest median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f860d485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected 136 outliers in 'Other'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_ranking(value):\n",
    "    if value in ['A*', 'A', 'B', 'C']:\n",
    "        return value  # CORE ranking\n",
    "    elif value in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "        return value  # Scimago quartile\n",
    "    # elif  in str(value):\n",
    "        # return 'National'\n",
    "    # elif value in ['National', 'Multiconference', 'TBR', 'Unranked', '-']:\n",
    "    #     return 'Other'\n",
    "    # elif value in ['Unranked', '-']:\n",
    "    #     return 'Unranked'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['venue_ranking'] = df['venue_ranking'].apply(normalize_ranking)\n",
    "\n",
    "venue_medians = (\n",
    "    df[df['venue_ranking'] != 'Other']\n",
    "    .groupby('venue_ranking')['citationCount']\n",
    "    .median()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# print(\"Median citations by venue:\")\n",
    "# print(venue_medians)\n",
    "\n",
    "# --- Step 2: Handle case when 'Other' is empty\n",
    "other_group = df[df['venue_ranking'] == 'Other']\n",
    "Q1 = np.percentile(other_group['citationCount'], 25)\n",
    "Q3 = np.percentile(other_group['citationCount'], 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_other = other_group[\n",
    "    (other_group['citationCount'] < lower_bound) |\n",
    "    (other_group['citationCount'] > upper_bound)\n",
    "]\n",
    "\n",
    "print(f\"\\nDetected {len(outliers_other)} outliers in 'Other'\")\n",
    "\n",
    "# --- Step 3: Reassign nearest median\n",
    "def nearest_venue(citation, medians):\n",
    "    return min(medians.keys(), key=lambda k: abs(medians[k] - citation))\n",
    "\n",
    "# df['venue_ranking_imputed'] = df['venue_ranking']\n",
    "\n",
    "for idx, row in outliers_other.iterrows():\n",
    "    nearest = nearest_venue(row['citationCount'], venue_medians)\n",
    "    df.at[idx, 'venue_ranking'] = nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7175ca9",
   "metadata": {},
   "source": [
    "### Number of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347c8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_authors(authors):\n",
    "    return len(authors)\n",
    "    \n",
    "df['num_authors'] = df['authors'].apply(count_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e5d94",
   "metadata": {},
   "source": [
    "### Add authors' statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19372f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_author_stats(authors_list):\n",
    "    if not authors_list:\n",
    "        return pd.Series({\n",
    "            \"mean_citations_all\": np.nan,\n",
    "            \"max_citations_all\": np.nan,\n",
    "            \"mean_h_index_all\": np.nan,\n",
    "            \"max_h_index_all\": np.nan,\n",
    "            \"mean_i10_index_all\": np.nan,\n",
    "            \"max_i10_index_all\": np.nan,\n",
    "        })\n",
    "    citations_all = [(a.get(\"citations_all\") or 0) for a in authors_list]\n",
    "    h_all = [(a.get(\"h_index_all\") or 0) for a in authors_list]\n",
    "    i10_all = [(a.get(\"i10_index_all\") or 0) for a in authors_list]\n",
    "    return pd.Series({\n",
    "        \"mean_citations_all\": np.mean(citations_all),\n",
    "        \"max_citations_all\": np.max(citations_all),\n",
    "        \"mean_h_index_all\": np.mean(h_all),\n",
    "        \"max_h_index_all\": np.max(h_all),\n",
    "        \"mean_i10_index_all\": np.mean(i10_all),\n",
    "        \"max_i10_index_all\": np.max(i10_all),\n",
    "    })\n",
    "\n",
    "author_features = df[\"authors\"].apply(extract_author_stats)\n",
    "df = pd.concat([df, author_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38266e61",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1263d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['venue_type'] = df['venue_type'].map({'preprint':0, 'conference':1, 'journal':2})\n",
    "df['venue_ranking'] = df['venue_ranking'].map({'Q4':1, 'Q3':2, 'Q2':3, 'Q1':4, 'C': 1, 'B':2, 'A':3, 'A*':4, 'Other':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fef9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create encoder\n",
    "le = LabelEncoder()\n",
    "df['primary_category'] = le.fit_transform(df['primary_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7763cc",
   "metadata": {},
   "source": [
    "### Add slope of trend for each category (based on number of papers and citations in each category over time)\n",
    "\n",
    "We don't consider the peak time (2020 in paper slope and 2023, 2024 in citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0dcf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "def safe_slope(x, y):\n",
    "    if len(x) < 2:\n",
    "        return 0\n",
    "    return linregress(x, y).slope\n",
    "\n",
    "# Exclude anomaly years for papers\n",
    "papers_trend = (\n",
    "    df[~df['published_year'].isin([2020])]\n",
    "    .groupby(['primary_category', 'published_year'])\n",
    "    .size()\n",
    "    .reset_index(name='num_papers')\n",
    ")\n",
    "\n",
    "# Exclude anomaly years for citations\n",
    "citations_trend = (\n",
    "    df[~df['published_year'].isin([2023, 2024, 2025])]\n",
    "    .groupby(['primary_category', 'published_year'])['citationCount']\n",
    "    .sum()\n",
    "    .reset_index(name='total_citations')\n",
    ")\n",
    "# display(citations_trend)\n",
    "# Compute slopes\n",
    "slope_papers = (\n",
    "    papers_trend.groupby('primary_category')\n",
    "    .apply(lambda g: safe_slope(g['published_year'], g['num_papers']), include_groups=False)\n",
    "    .reset_index(name='slope_papers')\n",
    ")\n",
    "# display(slope_papers)\n",
    "slope_citations = (\n",
    "    citations_trend.groupby('primary_category')\n",
    "    .apply(lambda g: safe_slope(g['published_year'], g['total_citations']), include_groups=False)\n",
    "    .reset_index(name='slope_citations')\n",
    ")\n",
    "# display(slope_citations)\n",
    "# Combine and sort\n",
    "trend_df = slope_papers.merge(slope_citations, on='primary_category', how='outer')\n",
    "trend_df = trend_df.sort_values('slope_citations', ascending=False)\n",
    "# display(trend_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208a5c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_revisions', 'primary_category', 'num_pages', 'github_stars',\n",
       "       'upvote', 'citing_models', 'citing_datasets', 'citing_spaces',\n",
       "       'citing_collections', 'citationCount', 'referenceCount',\n",
       "       'influentialCitationCount', 'venue_type', 'venue_ranking',\n",
       "       'published_year', 'num_authors', 'mean_citations_all',\n",
       "       'max_citations_all', 'mean_h_index_all', 'max_h_index_all',\n",
       "       'mean_i10_index_all', 'max_i10_index_all', 'slope_papers',\n",
       "       'slope_citations'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(trend_df, on=\"primary_category\", how=\"left\")\n",
    "numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "numeric_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fedb01",
   "metadata": {},
   "source": [
    "### Remove citations_by_years in 2024, 2025 to avoid data leakage\n",
    "citations in 2024 is the target we wanna predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_citation_count(row):\n",
    "    citations = row[\"citations_by_year\"]\n",
    "    if isinstance(citations, dict):\n",
    "        subtract = citations.get(\"2024\", 0) + citations.get(\"2025\", 0)\n",
    "        return row[\"citationCount\"] - subtract\n",
    "    return row[\"citationCount\"]\n",
    "\n",
    "df[\"citationCount\"] = df.apply(adjust_citation_count, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ca2d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"citations_2024\"] = df[\"citations_by_year\"].apply(\n",
    "    lambda x: x.get(\"2024\", 0) if isinstance(x, dict) else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82d28a",
   "metadata": {},
   "source": [
    "### Log transform citationCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a69992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"citationCount_log\"] = np.log1p(df[\"citationCount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e9b25",
   "metadata": {},
   "source": [
    "### Drop citationCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e992d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = \"citationCount\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f9d75",
   "metadata": {},
   "source": [
    "### Add num_years_after_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c9ed6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_years_after_publication'] = df['published_year'].apply(lambda x: 2025 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22763b1b",
   "metadata": {},
   "source": [
    "### Add statistics about citations_after_years (like mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3eac71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mean_citations_over_years'] = df['citations_after_years'].apply(lambda x: np.mean(list(x.values())))\n",
    "df['std_citations_over_years'] = df['citations_after_years'].apply(lambda x: np.std(list(x.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a35e8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e9e090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.drop(columns=['published_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8749a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c7bb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.to_csv(\"numeric_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
