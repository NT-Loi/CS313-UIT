{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfb0b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Scanning: data/2012\n",
      "üìÇ Scanning: data/2013\n",
      "üìÇ Scanning: data/2014\n",
      "üìÇ Scanning: data/2015\n",
      "üìÇ Scanning: data/2016\n",
      "üìÇ Scanning: data/2017\n",
      "üìÇ Scanning: data/2018\n",
      "üìÇ Scanning: data/2019\n",
      "üìÇ Scanning: data/2020\n",
      "üìÇ Scanning: data/2021\n",
      "üìÇ Scanning: data/2022\n",
      "üìÇ Scanning: data/2023\n",
      "üìÇ Scanning: data/2024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>published_date</th>\n",
       "      <th>last_revised_date</th>\n",
       "      <th>num_revisions</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>categories</th>\n",
       "      <th>...</th>\n",
       "      <th>citing_spaces</th>\n",
       "      <th>citing_collections</th>\n",
       "      <th>citations_by_year</th>\n",
       "      <th>citationCount</th>\n",
       "      <th>venue</th>\n",
       "      <th>citations</th>\n",
       "      <th>referenceCount</th>\n",
       "      <th>references</th>\n",
       "      <th>influentialCitationCount</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1212.2518</td>\n",
       "      <td>Efficient Inference in Large Discrete Domains</td>\n",
       "      <td>[{'name': 'R Sharma', 'citations_all': None, '...</td>\n",
       "      <td>In this paper we examine the problem of infere...</td>\n",
       "      <td>2012-10-19T00:00:00</td>\n",
       "      <td>2012-10-19T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>https://arxiv.org/pdf/1212.2518.pdf</td>\n",
       "      <td>Artificial Intelligence (cs.AI)</td>\n",
       "      <td>[Artificial Intelligence (cs.AI)]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'2003': 1, '2005': 1, '2006': 1, '2007': 1, '...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>{'name': 'Conference on Uncertainty in Artific...</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 26, 'cit...</td>\n",
       "      <td>13</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 9, 'cita...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1212.2511</td>\n",
       "      <td>Stochastic complexity of Bayesian networks</td>\n",
       "      <td>[{'name': 'K Yamazaki', 'citations_all': 708, ...</td>\n",
       "      <td>Bayesian networks are now being used in enormo...</td>\n",
       "      <td>2012-10-19T00:00:00</td>\n",
       "      <td>2012-10-19T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>https://arxiv.org/pdf/1212.2511.pdf</td>\n",
       "      <td>Machine Learning (cs.LG)</td>\n",
       "      <td>[Machine Learning (cs.LG), Machine Learning (s...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'2003': 2, '2004': 5, '2005': 6, '2006': 5, '...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>{'name': 'Conference on Uncertainty in Artific...</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 75, 'cit...</td>\n",
       "      <td>13</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 22, 'cit...</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1211.5625</td>\n",
       "      <td>A survey of computational methods for protein ...</td>\n",
       "      <td>[{'name': 'S Srihari', 'citations_all': 3135, ...</td>\n",
       "      <td>Complexes of physically interacting proteins a...</td>\n",
       "      <td>2012-11-24T00:00:00</td>\n",
       "      <td>2012-11-24T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>https://arxiv.org/pdf/1211.5625.pdf</td>\n",
       "      <td>Computational Engineering, Finance, and Scienc...</td>\n",
       "      <td>[Computational Engineering, Finance, and Scien...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'2013': 8, '2014': 11, '2015': 16, '2016': 14...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>{'name': None, 'type': None, 'ranking': None}</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 91, 'cit...</td>\n",
       "      <td>77</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 27, 'cit...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1212.2480</td>\n",
       "      <td>Approximate Inference and Constrained Optimiza...</td>\n",
       "      <td>[{'name': 'T Heskes', 'citations_all': 16337, ...</td>\n",
       "      <td>Loopy and generalized belief propagation are p...</td>\n",
       "      <td>2012-10-19T00:00:00</td>\n",
       "      <td>2012-10-19T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>https://arxiv.org/pdf/1212.2480.pdf</td>\n",
       "      <td>Machine Learning (cs.LG)</td>\n",
       "      <td>[Machine Learning (cs.LG), Artificial Intellig...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'2003': 3, '2004': 6, '2005': 8, '2006': 10, ...</td>\n",
       "      <td>143.0</td>\n",
       "      <td>{'name': 'Conference on Uncertainty in Artific...</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 77, 'cit...</td>\n",
       "      <td>13</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': 69, 'cit...</td>\n",
       "      <td>13</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1212.4674</td>\n",
       "      <td>Natural Language Understanding Based on Semant...</td>\n",
       "      <td>[{'name': 'H Kong', 'citations_all': None, 'ci...</td>\n",
       "      <td>In this paper, we define event expression over...</td>\n",
       "      <td>2012-12-19T00:00:00</td>\n",
       "      <td>2012-12-19T00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>https://arxiv.org/pdf/1212.4674.pdf</td>\n",
       "      <td>Computation and Language (cs.CL)</td>\n",
       "      <td>[Computation and Language (cs.CL)]</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'name': 'arXiv.org', 'type': None, 'ranking':...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'arxiv_id': None, 'referenceCount': None, 'c...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    arxiv_id                                              title  \\\n",
       "0  1212.2518      Efficient Inference in Large Discrete Domains   \n",
       "1  1212.2511         Stochastic complexity of Bayesian networks   \n",
       "2  1211.5625  A survey of computational methods for protein ...   \n",
       "3  1212.2480  Approximate Inference and Constrained Optimiza...   \n",
       "4  1212.4674  Natural Language Understanding Based on Semant...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'name': 'R Sharma', 'citations_all': None, '...   \n",
       "1  [{'name': 'K Yamazaki', 'citations_all': 708, ...   \n",
       "2  [{'name': 'S Srihari', 'citations_all': 3135, ...   \n",
       "3  [{'name': 'T Heskes', 'citations_all': 16337, ...   \n",
       "4  [{'name': 'H Kong', 'citations_all': None, 'ci...   \n",
       "\n",
       "                                            abstract       published_date  \\\n",
       "0  In this paper we examine the problem of infere...  2012-10-19T00:00:00   \n",
       "1  Bayesian networks are now being used in enormo...  2012-10-19T00:00:00   \n",
       "2  Complexes of physically interacting proteins a...  2012-11-24T00:00:00   \n",
       "3  Loopy and generalized belief propagation are p...  2012-10-19T00:00:00   \n",
       "4  In this paper, we define event expression over...  2012-12-19T00:00:00   \n",
       "\n",
       "     last_revised_date  num_revisions                              pdf_url  \\\n",
       "0  2012-10-19T00:00:00              0  https://arxiv.org/pdf/1212.2518.pdf   \n",
       "1  2012-10-19T00:00:00              0  https://arxiv.org/pdf/1212.2511.pdf   \n",
       "2  2012-11-24T00:00:00              0  https://arxiv.org/pdf/1211.5625.pdf   \n",
       "3  2012-10-19T00:00:00              0  https://arxiv.org/pdf/1212.2480.pdf   \n",
       "4  2012-12-19T00:00:00              0  https://arxiv.org/pdf/1212.4674.pdf   \n",
       "\n",
       "                                    primary_category  \\\n",
       "0                    Artificial Intelligence (cs.AI)   \n",
       "1                           Machine Learning (cs.LG)   \n",
       "2  Computational Engineering, Finance, and Scienc...   \n",
       "3                           Machine Learning (cs.LG)   \n",
       "4                   Computation and Language (cs.CL)   \n",
       "\n",
       "                                          categories  ... citing_spaces  \\\n",
       "0                  [Artificial Intelligence (cs.AI)]  ...             0   \n",
       "1  [Machine Learning (cs.LG), Machine Learning (s...  ...             0   \n",
       "2  [Computational Engineering, Finance, and Scien...  ...             0   \n",
       "3  [Machine Learning (cs.LG), Artificial Intellig...  ...             0   \n",
       "4                 [Computation and Language (cs.CL)]  ...             0   \n",
       "\n",
       "   citing_collections                                  citations_by_year  \\\n",
       "0                   0  {'2003': 1, '2005': 1, '2006': 1, '2007': 1, '...   \n",
       "1                   0  {'2003': 2, '2004': 5, '2005': 6, '2006': 5, '...   \n",
       "2                   0  {'2013': 8, '2014': 11, '2015': 16, '2016': 14...   \n",
       "3                   0  {'2003': 3, '2004': 6, '2005': 8, '2006': 10, ...   \n",
       "4                   0                                                 {}   \n",
       "\n",
       "   citationCount                                              venue  \\\n",
       "0           11.0  {'name': 'Conference on Uncertainty in Artific...   \n",
       "1           45.0  {'name': 'Conference on Uncertainty in Artific...   \n",
       "2          127.0      {'name': None, 'type': None, 'ranking': None}   \n",
       "3          143.0  {'name': 'Conference on Uncertainty in Artific...   \n",
       "4            0.0  {'name': 'arXiv.org', 'type': None, 'ranking':...   \n",
       "\n",
       "                                           citations  referenceCount  \\\n",
       "0  [{'arxiv_id': None, 'referenceCount': 26, 'cit...              13   \n",
       "1  [{'arxiv_id': None, 'referenceCount': 75, 'cit...              13   \n",
       "2  [{'arxiv_id': None, 'referenceCount': 91, 'cit...              77   \n",
       "3  [{'arxiv_id': None, 'referenceCount': 77, 'cit...              13   \n",
       "4                                                 []               5   \n",
       "\n",
       "                                          references influentialCitationCount  \\\n",
       "0  [{'arxiv_id': None, 'referenceCount': 9, 'cita...                        1   \n",
       "1  [{'arxiv_id': None, 'referenceCount': 22, 'cit...                        2   \n",
       "2  [{'arxiv_id': None, 'referenceCount': 27, 'cit...                        4   \n",
       "3  [{'arxiv_id': None, 'referenceCount': 69, 'cit...                       13   \n",
       "4  [{'arxiv_id': None, 'referenceCount': None, 'c...                        0   \n",
       "\n",
       "   embedding  \n",
       "0       None  \n",
       "1       None  \n",
       "2       None  \n",
       "3       None  \n",
       "4       None  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path(\"data\")\n",
    "\n",
    "# Folders you care about\n",
    "target_folders = [\"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"]\n",
    "\n",
    "records = []\n",
    "\n",
    "for folder in target_folders:\n",
    "    folder_path = root_dir / folder\n",
    "\n",
    "    print(f\"üìÇ Scanning: {folder_path}\")\n",
    "\n",
    "    # Loop through all json files in the folder\n",
    "    for json_file in folder_path.glob(\"*.json\"):\n",
    "\n",
    "        # Read JSON\n",
    "        try:\n",
    "            with open(json_file, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read {json_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        records.append(data)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88298e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_df = pd.json_normalize(df['venue'],\n",
    "                             sep='.')\n",
    "df = pd.concat([df, venue_df], axis=1)\n",
    "df.drop(columns=['venue'], inplace=True)\n",
    "df.rename(columns={'name': 'venue_name',\n",
    "                   'type': 'venue_type',\n",
    "                   'ranking': 'venue_ranking'},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1034da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['pdf_url', 'embedding', 'venue_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e88dd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_category(cat):\n",
    "    match = re.search(r'\\((.*?)\\)', cat)\n",
    "    return match.group(1) if match else cat.strip()\n",
    "\n",
    "df['categories'] = df['categories'].apply(lambda lst: [normalize_category(c) for c in lst])\n",
    "df['primary_category'] = df['primary_category'].apply(normalize_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c5205",
   "metadata": {},
   "source": [
    "### Remove paper with num_pages = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2343f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['num_pages'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6264f",
   "metadata": {},
   "source": [
    "### Drop keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9826b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='keywords', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b5bfb",
   "metadata": {},
   "source": [
    "### Remove paper with citations_by_year and citationCount = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6611517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['citations_by_year'].notna()]\n",
    "df = df[df['citationCount'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb592de",
   "metadata": {},
   "source": [
    "### Fill missing values in github_stars = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a715b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['github_stars'].isna(), 'github_stars'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65c6a8",
   "metadata": {},
   "source": [
    "### Fill missing (venue.type, venue.ranking) = (preprint, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f7729a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['venue_type'].isna(), 'venue_type'] = 'preprint'\n",
    "df.loc[df['venue_ranking'].isna(), 'venue_ranking'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0a1abc",
   "metadata": {},
   "source": [
    "### Add columns 'citations_after_years{0: ..., 1: ..., 2: ..., 3: ...}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12e54846",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['published_date'] = pd.to_datetime(df['published_date'])\n",
    "df['published_year'] = df['published_date'].dt.year\n",
    "\n",
    "# Iterate over each row to calculate citations after X years\n",
    "def calculate_citations_after_years(row):\n",
    "    citations_by_year = row['citations_by_year']\n",
    "    published_year = row['published_year']\n",
    "    result = {}\n",
    "    for year in range(published_year, 2025):\n",
    "        result[year-published_year] = 0\n",
    "    for year, count in citations_by_year.items():\n",
    "        year = int(year)\n",
    "        result[year - published_year] = count\n",
    "    return result\n",
    "\n",
    "# def reset_published_year(row):\n",
    "#     # citations = row.get(\"citations_by_year\", {})\n",
    "#     # if not citations:\n",
    "#     return row[\"published_date\"].year\n",
    "    \n",
    "#     # cited_years = [int(y) for y, c in citations.items()]\n",
    "#     # return min(cited_years)  # first year with citations\n",
    "    \n",
    "# # df[\"published_year\"] = df.apply(reset_published_year, axis=1)\n",
    "df['citations_after_years'] = df.apply(calculate_citations_after_years, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb42a23",
   "metadata": {},
   "source": [
    "### Assign outliers' venue ranking by their nearest median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860d485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected 1276 outliers in 'Other'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_ranking(value):\n",
    "    if value in ['A*', 'A', 'B', 'C']:\n",
    "        return value  # CORE ranking\n",
    "    elif value in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "        return value  # Scimago quartile\n",
    "    # elif  in str(value):\n",
    "        # return 'National'\n",
    "    # elif value in ['National', 'Multiconference', 'TBR', 'Unranked', '-']:\n",
    "    #     return 'Other'\n",
    "    # elif value in ['Unranked', '-']:\n",
    "    #     return 'Unranked'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['venue_ranking'] = df['venue_ranking'].apply(normalize_ranking)\n",
    "\n",
    "venue_medians = (\n",
    "    df[df['venue_ranking'] != 'Other']\n",
    "    .groupby('venue_ranking')['citationCount']\n",
    "    .median()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# print(\"Median citations by venue:\")\n",
    "# print(venue_medians)\n",
    "\n",
    "# --- Step 2: Handle case when 'Other' is empty\n",
    "other_group = df[df['venue_ranking'] == 'Other']\n",
    "Q1 = np.percentile(other_group['citationCount'], 25)\n",
    "Q3 = np.percentile(other_group['citationCount'], 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers_other = other_group[\n",
    "    (other_group['citationCount'] < lower_bound) |\n",
    "    (other_group['citationCount'] > upper_bound)\n",
    "]\n",
    "\n",
    "print(f\"\\nDetected {len(outliers_other)} outliers in 'Other'\")\n",
    "\n",
    "# --- Step 3: Reassign nearest median\n",
    "def nearest_venue(citation, medians):\n",
    "    return min(medians.keys(), key=lambda k: abs(medians[k] - citation))\n",
    "\n",
    "# df['venue_ranking_imputed'] = df['venue_ranking']\n",
    "\n",
    "for idx, row in outliers_other.iterrows():\n",
    "    nearest = nearest_venue(row['citationCount'], venue_medians)\n",
    "    df.at[idx, 'venue_ranking'] = nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7175ca9",
   "metadata": {},
   "source": [
    "### Number of authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_authors(authors):\n",
    "    return len(authors)\n",
    "    \n",
    "df['num_authors'] = df['authors'].apply(count_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e5d94",
   "metadata": {},
   "source": [
    "### Add authors' statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19372f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_author_stats(authors_list):\n",
    "    if not authors_list:\n",
    "        return pd.Series({\n",
    "            \"mean_citations_all\": np.nan,\n",
    "            \"max_citations_all\": np.nan,\n",
    "            \"mean_h_index_all\": np.nan,\n",
    "            \"max_h_index_all\": np.nan,\n",
    "            \"mean_i10_index_all\": np.nan,\n",
    "            \"max_i10_index_all\": np.nan,\n",
    "        })\n",
    "    citations_all = [(a.get(\"citations_all\") or 0) for a in authors_list]\n",
    "    h_all = [(a.get(\"h_index_all\") or 0) for a in authors_list]\n",
    "    i10_all = [(a.get(\"i10_index_all\") or 0) for a in authors_list]\n",
    "    return pd.Series({\n",
    "        \"mean_citations_all\": np.mean(citations_all),\n",
    "        \"max_citations_all\": np.max(citations_all),\n",
    "        \"mean_h_index_all\": np.mean(h_all),\n",
    "        \"max_h_index_all\": np.max(h_all),\n",
    "        \"mean_i10_index_all\": np.mean(i10_all),\n",
    "        \"max_i10_index_all\": np.max(i10_all),\n",
    "    })\n",
    "\n",
    "author_features = df[\"authors\"].apply(extract_author_stats)\n",
    "df = pd.concat([df, author_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38266e61",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['venue_type'] = df['venue_type'].map({'preprint':0, 'conference':1, 'journal':2})\n",
    "df['venue_ranking'] = df['venue_ranking'].map({'Q4':1, 'Q3':2, 'Q2':3, 'Q1':4, 'C': 1, 'B':2, 'A':3, 'A*':4, 'Other':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create encoder\n",
    "le = LabelEncoder()\n",
    "df['primary_category'] = le.fit_transform(df['primary_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7763cc",
   "metadata": {},
   "source": [
    "### Add slope of trend for each category (based on number of papers and citations in each category over time)\n",
    "\n",
    "We don't consider the peak time (2020 in paper slope and 2023, 2024 in citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dcf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "def safe_slope(x, y):\n",
    "    if len(x) < 2:\n",
    "        return 0\n",
    "    return linregress(x, y).slope\n",
    "\n",
    "# Exclude anomaly years for papers\n",
    "papers_trend = (\n",
    "    df[~df['published_year'].isin([2020])]\n",
    "    .groupby(['primary_category', 'published_year'])\n",
    "    .size()\n",
    "    .reset_index(name='num_papers')\n",
    ")\n",
    "\n",
    "# Exclude anomaly years for citations\n",
    "citations_trend = (\n",
    "    df[~df['published_year'].isin([2023, 2024, 2025])]\n",
    "    .groupby(['primary_category', 'published_year'])['citationCount']\n",
    "    .sum()\n",
    "    .reset_index(name='total_citations')\n",
    ")\n",
    "# display(citations_trend)\n",
    "# Compute slopes\n",
    "slope_papers = (\n",
    "    papers_trend.groupby('primary_category')\n",
    "    .apply(lambda g: safe_slope(g['published_year'], g['num_papers']), include_groups=False)\n",
    "    .reset_index(name='slope_papers')\n",
    ")\n",
    "# display(slope_papers)\n",
    "slope_citations = (\n",
    "    citations_trend.groupby('primary_category')\n",
    "    .apply(lambda g: safe_slope(g['published_year'], g['total_citations']), include_groups=False)\n",
    "    .reset_index(name='slope_citations')\n",
    ")\n",
    "# display(slope_citations)\n",
    "# Combine and sort\n",
    "trend_df = slope_papers.merge(slope_citations, on='primary_category', how='outer')\n",
    "trend_df = trend_df.sort_values('slope_citations', ascending=False)\n",
    "# display(trend_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a5c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_revisions', 'primary_category', 'num_pages', 'github_stars',\n",
       "       'upvote', 'citing_models', 'citing_datasets', 'citing_spaces',\n",
       "       'citing_collections', 'citationCount', 'referenceCount',\n",
       "       'influentialCitationCount', 'venue_type', 'venue_ranking',\n",
       "       'published_year', 'num_authors', 'mean_citations_all',\n",
       "       'max_citations_all', 'mean_h_index_all', 'max_h_index_all',\n",
       "       'mean_i10_index_all', 'max_i10_index_all', 'slope_papers',\n",
       "       'slope_citations'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(trend_df, on=\"primary_category\", how=\"left\")\n",
    "numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "numeric_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fedb01",
   "metadata": {},
   "source": [
    "### Remove citations_by_years in 2024, 2025 to avoid data leakage\n",
    "citations in 2024 is the target we wanna predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_citation_count(row):\n",
    "    citations = row[\"citations_by_year\"]\n",
    "    if isinstance(citations, dict):\n",
    "        subtract = citations.get(\"2024\", 0) + citations.get(\"2025\", 0)\n",
    "        return row[\"citationCount\"] - subtract\n",
    "    return row[\"citationCount\"]\n",
    "\n",
    "df[\"citationCount\"] = df.apply(adjust_citation_count, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2d13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"citations_2024\"] = df[\"citations_by_year\"].apply(\n",
    "    lambda x: x.get(\"2024\", 0) if isinstance(x, dict) else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e82d28a",
   "metadata": {},
   "source": [
    "### Log transform citationCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"citationCount_log\"] = np.log1p(df[\"citationCount\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e9b25",
   "metadata": {},
   "source": [
    "### Drop citationCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e992d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = \"citationCount\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f9d75",
   "metadata": {},
   "source": [
    "### Add num_years_after_publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ed6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_years_after_publication'] = df['published_year'].apply(lambda x: 2025 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22763b1b",
   "metadata": {},
   "source": [
    "### Add statistics about citations_after_years (mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eac71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nt-loi/CS313-UIT/.venv/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/nt-loi/CS313-UIT/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/nt-loi/CS313-UIT/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:223: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/nt-loi/CS313-UIT/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:181: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/nt-loi/CS313-UIT/.venv/lib/python3.10/site-packages/numpy/_core/_methods.py:215: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "df['mean_citations_over_years'] = df['citations_after_years'].apply(lambda x: np.mean(list(x.values())))\n",
    "df['std_citations_over_years'] = df['citations_after_years'].apply(lambda x: np.std(list(x.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.drop(columns=['published_year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df.to_csv(\"numeric_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
