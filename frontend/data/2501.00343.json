{
    "arxiv_id": "2501.00343",
    "title": "Chunk-Distilled Language Modeling",
    "authors": [
        {
            "name": "Y Li",
            "citations_all": 65,
            "citations_recent": 65,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 3,
            "i10_index_recent": 3
        },
        {
            "name": "K Livescu",
            "citations_all": 17173,
            "citations_recent": 11196,
            "h_index_all": 57,
            "h_index_recent": 46,
            "i10_index_all": 130,
            "i10_index_recent": 92
        },
        {
            "name": "J Zhou",
            "citations_all": 910,
            "citations_recent": 901,
            "h_index_all": 13,
            "h_index_recent": 13,
            "i10_index_all": 14,
            "i10_index_recent": 14
        }
    ],
    "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream tasks. Code and data will be made publicly available.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00343.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 39,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 3
    },
    "citationCount": 3,
    "venue": {
        "name": "International Conference on Learning Representations",
        "type": "conference"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 109,
            "citationCount": 5,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}