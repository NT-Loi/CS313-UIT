{
    "arxiv_id": "2501.00015",
    "title": "Energy-Efficient Sampling Using Stochastic Magnetic Tunnel Junctions",
    "authors": [
        {
            "name": "N Alder",
            "citations_all": 57,
            "citations_recent": 57,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "SN Kajale",
            "citations_all": 204,
            "citations_recent": 203,
            "h_index_all": 5,
            "h_index_recent": 5,
            "i10_index_all": 4,
            "i10_index_recent": 4
        },
        {
            "name": "M Tunsiricharoengul",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "D Sarkar",
            "citations_all": 8419,
            "citations_recent": 4759,
            "h_index_all": 32,
            "h_index_recent": 27,
            "i10_index_all": 44,
            "i10_index_recent": 37
        },
        {
            "name": "R Herbrich",
            "citations_all": 18892,
            "citations_recent": 6397,
            "h_index_all": 59,
            "h_index_recent": 37,
            "i10_index_all": 129,
            "i10_index_recent": 75
        }
    ],
    "abstract": "(Pseudo)random sampling, a costly yet widely used method in (probabilistic) machine learning and Markov Chain Monte Carlo algorithms, remains unfeasible on a truly large scale due to unmet computational requirements. We introduce an energy-efficient algorithm for uniform Float16 sampling, utilizing a room-temperature stochastic magnetic tunnel junction device to generate truly random floating-point numbers. By avoiding expensive symbolic computation and mapping physical phenomena directly to the statistical properties of the floating-point format and uniform distribution, our approach achieves a higher level of energy efficiency than the state-of-the-art Mersenne-Twister algorithm by a minimum factor of 9721 and an improvement factor of 5649 compared to the more energy-efficient PCG algorithm. Building on this sampling technique and hardware framework, we decompose arbitrary distributions into many non-overlapping approximative uniform distributions along with convolution and prior-likelihood operations, which allows us to sample from any 1D distribution without closed-form solutions. We provide measurements of the potential accumulated approximation errors, demonstrating the effectiveness of our method.",
    "published_date": "2024-12-14T00:00:00",
    "last_revised_date": "2024-12-14T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00015.pdf",
    "primary_category": "Computational Physics (physics.comp-ph)",
    "categories": [
        "Computational Physics (physics.comp-ph)",
        "Machine Learning (cs.LG)",
        "Computation (stat.CO)",
        "Machine Learning (stat.ML)"
    ],
    "keywords": null,
    "num_pages": 24,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}