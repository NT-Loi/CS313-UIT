{
    "arxiv_id": "2412.19834",
    "title": "RoboSignature: Robust Signature and Watermarking on Network Attacks",
    "authors": [
        {
            "name": "A Shaan",
            "citations_all": 7,
            "citations_recent": 7,
            "h_index_all": 1,
            "h_index_recent": 1,
            "i10_index_all": 0,
            "i10_index_recent": 0
        },
        {
            "name": "G Banga",
            "citations_all": 50,
            "citations_recent": 50,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "R Mantri",
            "citations_all": 7,
            "citations_recent": 7,
            "h_index_all": 2,
            "h_index_recent": 2,
            "i10_index_all": 0,
            "i10_index_recent": 0
        }
    ],
    "abstract": "Generative models have enabled easy creation and generation of images of all kinds given a single prompt. However, this has also raised ethical concerns about what is an actual piece of content created by humans or cameras compared to model-generated content like images or videos. Watermarking data generated by modern generative models is a popular method to provide information on the source of the content. The goal is for all generated images to conceal an invisible watermark, allowing for future detection or identification. The Stable Signature finetunes the decoder of Latent Diffusion Models such that a unique watermark is rooted in any image produced by the decoder. In this paper, we present a novel adversarial fine-tuning attack that disrupts the model's ability to embed the intended watermark, exposing a significant vulnerability in existing watermarking methods. To address this, we further propose a tamper-resistant fine-tuning algorithm inspired by methods developed for large language models, tailored to the specific requirements of watermarking in LDMs. Our findings emphasize the importance of anticipating and defending against potential vulnerabilities in generative systems.",
    "published_date": "2024-12-22T00:00:00",
    "last_revised_date": "2024-12-22T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19834.pdf",
    "primary_category": "Cryptography and Security (cs.CR)",
    "categories": [
        "Cryptography and Security (cs.CR)",
        "Artificial Intelligence (cs.AI)",
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 5,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}