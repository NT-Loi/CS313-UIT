{
    "arxiv_id": "2412.19200",
    "title": "Personalized Dynamic Music Emotion Recognition with Dual-Scale Attention-Based Meta-Learning",
    "authors": [
        {
            "name": "D Zhang",
            "citations_all": 41,
            "citations_recent": 41,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "W You",
            "citations_all": 187,
            "citations_recent": 185,
            "h_index_all": 9,
            "h_index_recent": 9,
            "i10_index_all": 6,
            "i10_index_recent": 6
        },
        {
            "name": "Z Liu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "L Sun",
            "citations_all": 3686,
            "citations_recent": 3345,
            "h_index_all": 30,
            "h_index_recent": 27,
            "i10_index_all": 96,
            "i10_index_recent": 93
        },
        {
            "name": "P Chen",
            "citations_all": 164,
            "citations_recent": 164,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 4,
            "i10_index_recent": 4
        }
    ],
    "abstract": "Dynamic Music Emotion Recognition (DMER) aims to predict the emotion of different moments in music, playing a crucial role in music information retrieval. The existing DMER methods struggle to capture long-term dependencies when dealing with sequence data, which limits their performance. Furthermore, these methods often overlook the influence of individual differences on emotion perception, even though everyone has their own personalized emotional perception in the real world. Motivated by these issues, we explore more effective sequence processing methods and introduce the Personalized DMER (PDMER) problem, which requires models to predict emotions that align with personalized perception. Specifically, we propose a Dual-Scale Attention-Based Meta-Learning (DSAML) method. This method fuses features from a dual-scale feature extractor and captures both short and long-term dependencies using a dual-scale attention transformer, improving the performance in traditional DMER. To achieve PDMER, we design a novel task construction strategy that divides tasks by annotators. Samples in a task are annotated by the same annotator, ensuring consistent perception. Leveraging this strategy alongside meta-learning, DSAML can predict personalized perception of emotions with just one personalized annotation sample. Our objective and subjective experiments demonstrate that our method can achieve state-of-the-art performance in both traditional DMER and PDMER.",
    "published_date": "2024-12-26T00:00:00",
    "last_revised_date": "2024-12-26T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19200.pdf",
    "primary_category": "Sound (cs.SD)",
    "categories": [
        "Sound (cs.SD)",
        "Information Retrieval (cs.IR)",
        "Audio and Speech Processing (eess.AS)"
    ],
    "keywords": null,
    "num_pages": 9,
    "github_stars": 11,
    "upvote": 1,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 1,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}