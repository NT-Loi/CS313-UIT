{
    "arxiv_id": "2412.19396",
    "title": "Comparing Few to Rank Many: Active Human Preference Learning using Randomized Frank-Wolfe",
    "authors": [
        {
            "name": "KK Thekumparampil",
            "citations_all": 1668,
            "citations_recent": 1564,
            "h_index_all": 15,
            "h_index_recent": 14,
            "i10_index_all": 16,
            "i10_index_recent": 15
        },
        {
            "name": "G Hiranandani",
            "citations_all": 616,
            "citations_recent": 568,
            "h_index_all": 16,
            "h_index_recent": 15,
            "i10_index_all": 20,
            "i10_index_recent": 19
        },
        {
            "name": "K Kalantari",
            "citations_all": 156,
            "citations_recent": 119,
            "h_index_all": 5,
            "h_index_recent": 5,
            "i10_index_all": 5,
            "i10_index_recent": 4
        },
        {
            "name": "S Sabach",
            "citations_all": 5762,
            "citations_recent": 3737,
            "h_index_all": 29,
            "h_index_recent": 28,
            "i10_index_all": 39,
            "i10_index_recent": 38
        },
        {
            "name": "B Kveton",
            "citations_all": 6235,
            "citations_recent": 4198,
            "h_index_all": 43,
            "h_index_recent": 36,
            "i10_index_all": 103,
            "i10_index_recent": 82
        }
    ],
    "abstract": "We study learning of human preferences from a limited comparison feedback. This task is ubiquitous in machine learning. Its applications such as reinforcement learning from human feedback, have been transformational. We formulate this problem as learning a Plackett-Luce model over a universe of $N$ choices from $K$-way comparison feedback, where typically $K \\ll N$. Our solution is the D-optimal design for the Plackett-Luce objective. The design defines a data logging policy that elicits comparison feedback for a small collection of optimally chosen points from all ${N \\choose K}$ feasible subsets. The main algorithmic challenge in this work is that even fast methods for solving D-optimal designs would have $O({N \\choose K})$ time complexity. To address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that solves the linear maximization sub-problems in the FW method on randomly chosen variables. We analyze the algorithm, and evaluate it empirically on synthetic and open-source NLP datasets.",
    "published_date": "2024-12-27T00:00:00",
    "last_revised_date": "2024-12-27T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19396.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Information Theory (cs.IT)",
        "Optimization and Control (math.OC)",
        "Machine Learning (stat.ML)"
    ],
    "keywords": null,
    "num_pages": 18,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 1
    },
    "citationCount": 1,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 49,
            "citationCount": 2,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 54,
            "citationCount": 2,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}