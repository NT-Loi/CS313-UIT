{
    "arxiv_id": "2412.20105",
    "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming",
    "authors": [
        {
            "name": "J Zhuang",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "L Lu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "M Dai",
            "citations_all": 573,
            "citations_recent": 573,
            "h_index_all": 9,
            "h_index_recent": 9,
            "i10_index_all": 9,
            "i10_index_recent": 9
        },
        {
            "name": "R Hu",
            "citations_all": 92,
            "citations_recent": 92,
            "h_index_all": 6,
            "h_index_recent": 6,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "J Chen",
            "citations_all": 640,
            "citations_recent": 262,
            "h_index_all": 12,
            "h_index_recent": 10,
            "i10_index_all": 16,
            "i10_index_recent": 11
        },
        {
            "name": "Q Liu",
            "citations_all": 3230,
            "citations_recent": 2628,
            "h_index_all": 23,
            "h_index_recent": 19,
            "i10_index_all": 54,
            "i10_index_recent": 29
        },
        {
            "name": "H Hu",
            "citations_all": 1983,
            "citations_recent": 1582,
            "h_index_all": 26,
            "h_index_recent": 24,
            "i10_index_all": 55,
            "i10_index_recent": 46
        }
    ],
    "abstract": "Multimodal large language models (MLLMs) enhance their perceptual capabilities by integrating visual and textual information. However, processing the massive number of visual tokens incurs a significant computational cost. Existing analysis of the MLLM attention mechanisms remains shallow, leading to coarse-grain token pruning strategies that fail to effectively balance speed and accuracy. In this paper, we conduct a comprehensive investigation of MLLM attention mechanisms with LLaVA. We find that numerous visual tokens and partial attention computations are redundant during the decoding process. Based on this insight, we propose Spatial-Temporal Visual Token Trimming ($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without retraining. $\\textbf{ST}^{3}$ consists of two primary components: 1) Progressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive visual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}), which dynamically reduces the number of visual tokens in each layer as the generated tokens grow. Together, these techniques deliver around $\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache memory compared to the original LLaVA, while maintaining consistent performance across various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly integrated into existing pre-trained MLLMs, providing a plug-and-play solution for efficient inference.",
    "published_date": "2024-12-28T00:00:00",
    "last_revised_date": "2024-12-28T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20105.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "keywords": null,
    "num_pages": 17,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 8,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 84,
            "citationCount": 3,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 27,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 243,
            "citationCount": 5,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 49,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 45,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 170,
            "citationCount": 15,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 48,
            "citationCount": 11,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 54,
            "citationCount": 21,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}