{
    "arxiv_id": "2501.00039",
    "title": "Speech Recognition With LLMs Adapted to Disordered Speech Using Reinforcement Learning",
    "authors": [
        {
            "name": "C Nagpal",
            "citations_all": 1771,
            "citations_recent": 1744,
            "h_index_all": 16,
            "h_index_recent": 16,
            "i10_index_all": 20,
            "i10_index_recent": 20
        },
        {
            "name": "S Venugopalan",
            "citations_all": 26090,
            "citations_recent": 17644,
            "h_index_all": 31,
            "h_index_recent": 30,
            "i10_index_all": 42,
            "i10_index_recent": 42
        },
        {
            "name": "J Tobin",
            "citations_all": 352,
            "citations_recent": 349,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 8,
            "i10_index_recent": 8
        },
        {
            "name": "M Ladewig",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "K Heller",
            "citations_all": 9455,
            "citations_recent": 6701,
            "h_index_all": 51,
            "h_index_recent": 38,
            "i10_index_all": 82,
            "i10_index_recent": 68
        },
        {
            "name": "K Tomanek",
            "citations_all": 2503,
            "citations_recent": 1236,
            "h_index_all": 26,
            "h_index_recent": 18,
            "i10_index_all": 43,
            "i10_index_recent": 33
        }
    ],
    "abstract": "We introduce a large language model (LLM) capable of processing speech inputs and show that tuning it further with reinforcement learning on human preference (RLHF) enables it to adapt better to disordered speech than traditional fine-tuning. Our method replaces low-frequency text tokens in an LLM's vocabulary with audio tokens and enables the model to recognize speech by fine-tuning it on speech with transcripts. We then use RL with rewards based on syntactic and semantic accuracy measures generalizing the LLM further to recognize disordered speech. While the resulting LLM does not outperform existing systems for speech recognition, we find that tuning with reinforcement learning using custom rewards leads to substantially better performance than supervised fine-tuning of the language model, specifically when adapting to speech in a different setting. This presents a compelling alternative tuning strategy for speech recognition using large language models.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00039.pdf",
    "primary_category": "Audio and Speech Processing (eess.AS)",
    "categories": [
        "Audio and Speech Processing (eess.AS)",
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)",
        "Sound (cs.SD)"
    ],
    "keywords": null,
    "num_pages": 5,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 3,
    "venue": {
        "name": null,
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}