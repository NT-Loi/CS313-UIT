{
    "arxiv_id": "2412.18579",
    "title": "ReducedLUT: Table Decomposition with \"Don't Care\" Conditions",
    "authors": [
        {
            "name": "O Cassidy",
            "citations_all": 2,
            "citations_recent": 2,
            "h_index_all": 1,
            "h_index_recent": 1,
            "i10_index_all": 0,
            "i10_index_recent": 0
        },
        {
            "name": "M Andronic",
            "citations_all": 65,
            "citations_recent": 65,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 3,
            "i10_index_recent": 3
        },
        {
            "name": "S Coward",
            "citations_all": 174,
            "citations_recent": 174,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 6,
            "i10_index_recent": 6
        },
        {
            "name": "GA Constantinides",
            "citations_all": 8810,
            "citations_recent": 3195,
            "h_index_all": 46,
            "h_index_recent": 28,
            "i10_index_all": 188,
            "i10_index_recent": 84
        }
    ],
    "abstract": "Lookup tables (LUTs) are frequently used to efficiently store arrays of precomputed values for complex mathematical computations. When used in the context of neural networks, these functions exhibit a lack of recognizable patterns which presents an unusual challenge for conventional logic synthesis techniques. Several approaches are known to break down a single large lookup table into multiple smaller ones that can be recombined. Traditional methods, such as plain tabulation, piecewise linear approximation, and multipartite table methods, often yield inefficient hardware solutions when applied to LUT-based NNs.\nThis paper introduces ReducedLUT, a novel method to reduce the footprint of the LUTs by injecting don't cares into the compression process. This additional freedom introduces more self-similarities which can be exploited using known decomposition techniques. We then demonstrate a particular application to machine learning; by replacing unobserved patterns within the training data of neural network models with don't cares, we enable greater compression with minimal model accuracy degradation. In practice, we achieve up to $1.63\\times$ reduction in Physical LUT utilization, with a test accuracy drop of no more than $0.01$ accuracy points.",
    "published_date": "2024-12-24T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 1,
    "pdf_url": "https://arxiv.org/pdf/2412.18579.pdf",
    "primary_category": "Hardware Architecture (cs.AR)",
    "categories": [
        "Hardware Architecture (cs.AR)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 7,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 2,
    "venue": {
        "name": "Symposium on Field Programmable Gate Arrays",
        "type": "conference"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 30,
            "citationCount": 3,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 14,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 18,
            "citationCount": 3,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 20,
            "citationCount": 5,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 8,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 21,
            "citationCount": 27,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 257,
            "citationCount": 347,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 24,
            "citationCount": 99,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 13,
            "citationCount": 23,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 47,
            "citationCount": 939,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 19,
            "citationCount": 97,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 19,
            "citationCount": 197,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 26,
            "citationCount": 203,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 21,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}