{
    "arxiv_id": "2501.00432",
    "title": "OV-HHIR: Open Vocabulary Human Interaction Recognition Using Cross-modal Integration of Large Language Models",
    "authors": [
        {
            "name": "Y Huang",
            "citations_all": 511,
            "citations_recent": 470,
            "h_index_all": 11,
            "h_index_recent": 10,
            "i10_index_all": 12,
            "i10_index_recent": 10
        },
        {
            "name": "K Kastner",
            "citations_all": 4191,
            "citations_recent": 3045,
            "h_index_all": 13,
            "h_index_recent": 13,
            "i10_index_all": 13,
            "i10_index_recent": 13
        },
        {
            "name": "K Audhkhasi",
            "citations_all": 4597,
            "citations_recent": 3125,
            "h_index_all": 33,
            "h_index_recent": 29,
            "i10_index_all": 66,
            "i10_index_recent": 50
        },
        {
            "name": "B Ramabhadran",
            "citations_all": 17366,
            "citations_recent": 8531,
            "h_index_all": 61,
            "h_index_recent": 44,
            "i10_index_all": 201,
            "i10_index_recent": 119
        },
        {
            "name": "A Rosenberg",
            "citations_all": 7758,
            "citations_recent": 4985,
            "h_index_all": 38,
            "h_index_recent": 29,
            "i10_index_all": 91,
            "i10_index_recent": 62
        }
    ],
    "abstract": "Understanding human-to-human interactions, especially in contexts like public security surveillance, is critical for monitoring and maintaining safety. Traditional activity recognition systems are limited by fixed vocabularies, predefined labels, and rigid interaction categories that often rely on choreographed videos and overlook concurrent interactive groups. These limitations make such systems less adaptable to real-world scenarios, where interactions are diverse and unpredictable. In this paper, we propose an open vocabulary human-to-human interaction recognition (OV-HHIR) framework that leverages large language models to generate open-ended textual descriptions of both seen and unseen human interactions in open-world settings without being confined to a fixed vocabulary. Additionally, we create a comprehensive, large-scale human-to-human interaction dataset by standardizing and combining existing public human interaction datasets into a unified benchmark. Extensive experiments demonstrate that our method outperforms traditional fixed-vocabulary classification systems and existing cross-modal language models for video understanding, setting the stage for more intelligent and adaptable visual understanding systems in surveillance and beyond.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00432.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 5,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 4,
    "venue": {
        "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "type": "conference"
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}