{
    "arxiv_id": "2501.10368",
    "title": "The Potential of Answer Classes in Large-scale Written Computer-Science Exams -- Vol. 2",
    "authors": [
        {
            "name": "D Lohr",
            "citations_all": 288,
            "citations_recent": 288,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 7,
            "i10_index_recent": 7
        },
        {
            "name": "M Berges",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "M Kohlhase",
            "citations_all": 7900,
            "citations_recent": 1321,
            "h_index_all": 43,
            "h_index_recent": 16,
            "i10_index_all": 167,
            "i10_index_recent": 41
        },
        {
            "name": "F Rabe",
            "citations_all": 2003,
            "citations_recent": 594,
            "h_index_all": 24,
            "h_index_recent": 13,
            "i10_index_all": 63,
            "i10_index_recent": 21
        }
    ],
    "abstract": "Students' answers to tasks provide a valuable source of information in teaching as they result from applying cognitive processes to a learning content addressed in the task. Due to steadily increasing course sizes, analyzing student answers is frequently the only means of obtaining evidence about student performance. However, in many cases, resources are limited, and when evaluating exams, the focus is solely on identifying correct or incorrect answers. This overlooks the value of analyzing incorrect answers, which can help improve teaching strategies or identify misconceptions to be addressed in the next cohort.\nIn teacher training for secondary education, assessment guidelines are mandatory for every exam, including anticipated errors and misconceptions. We applied this concept to a university exam with 462 students and 41 tasks. For each task, the instructors developed answer classes -- classes of expected responses, to which student answers were mapped during the exam correction process. The experiment resulted in a shift in mindset among the tutors and instructors responsible for the course: after initially having great reservations about whether the significant additional effort would yield an appropriate benefit, the procedure was subsequently found to be extremely valuable.\nThe concept presented, and the experience gained from the experiment were cast into a system with which it is possible to correct paper-based exams on the basis of answer classes. This updated version of the paper provides an overview and new potential in the course of using the digital version of the approach.",
    "published_date": "2024-12-12T00:00:00",
    "last_revised_date": "2024-12-12T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.10368.pdf",
    "primary_category": "Computers and Society (cs.CY)",
    "categories": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": [
        "CSE task analysis answer classes assessment"
    ],
    "num_pages": 19,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 13,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}