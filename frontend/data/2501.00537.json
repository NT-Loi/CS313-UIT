{
    "arxiv_id": "2501.00537",
    "title": "Extending XReason: Formal Explanations for Adversarial Detection",
    "authors": [
        {
            "name": "A Jemaa",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "A Rashid",
            "citations_all": 364,
            "citations_recent": 304,
            "h_index_all": 11,
            "h_index_recent": 9,
            "i10_index_all": 11,
            "i10_index_recent": 8
        },
        {
            "name": "S Tahar",
            "citations_all": 6108,
            "citations_recent": 1816,
            "h_index_all": 37,
            "h_index_recent": 19,
            "i10_index_all": 162,
            "i10_index_recent": 45
        }
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) plays an important role in improving the transparency and reliability of complex machine learning models, especially in critical domains such as cybersecurity. Despite the prevalence of heuristic interpretation methods such as SHAP and LIME, these techniques often lack formal guarantees and may produce inconsistent local explanations. To fulfill this need, few tools have emerged that use formal methods to provide formal explanations. Among these, XReason uses a SAT solver to generate formal instance-level explanation for XGBoost models. In this paper, we extend the XReason tool to support LightGBM models as well as class-level explanations. Additionally, we implement a mechanism to generate and detect adversarial examples in XReason. We evaluate the efficiency and accuracy of our approach on the CICIDS-2017 dataset, a widely used benchmark for detecting network attacks.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00537.pdf",
    "primary_category": "Artificial Intelligence (cs.AI)",
    "categories": [
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": [
        "Explainable AI",
        "LightGBM",
        "Formal Explanations",
        "Adversarial Robustness",
        "XReason"
    ],
    "num_pages": 10,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}