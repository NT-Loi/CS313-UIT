{
    "arxiv_id": "2501.00522",
    "title": "TinyHelen's First Curriculum: Training and Evaluating Tiny Language Models in a Simpler Language Environment",
    "authors": [
        {
            "name": "K Yang",
            "citations_all": 281,
            "citations_recent": 281,
            "h_index_all": 5,
            "h_index_recent": 5,
            "i10_index_all": 5,
            "i10_index_recent": 5
        },
        {
            "name": "V Kindratenko",
            "citations_all": 3640,
            "citations_recent": 1232,
            "h_index_all": 31,
            "h_index_recent": 19,
            "i10_index_all": 65,
            "i10_index_recent": 30
        },
        {
            "name": "CX Zhai",
            "citations_all": 45662,
            "citations_recent": 14220,
            "h_index_all": 96,
            "h_index_recent": 54,
            "i10_index_all": 336,
            "i10_index_recent": 203
        }
    ],
    "abstract": "Training language models (LMs) and their application agents is increasingly costly due to large datasets and models, making test failures difficult to bear. Simplified language environments serve as primordial training and testing grounds, retaining essential commonsense and communication skills but in a more digestible form, potentially enhancing the learning efficiency of LMs, and thus reducing the required model size and data volume for effective training and evaluation. In these simplified language environments, workable strategies for small models, datasets, and agents may be adaptable to larger models, datasets, and agents in complex language environments.\nTo create such environments, we focus on two aspects: i) minimizing language dataset noise and complexity, and ii) preserving the essential text distribution characteristics. Unlike previous methods, we propose a pipeline to refine text data by eliminating noise, minimizing vocabulary, and maintaining genre-specific patterns (e.g., for books, conversation, code, etc.). Implementing this pipeline with large LMs, we have created a leaner suite of LM training and evaluation datasets: 71M Leaner-Pretrain, 7M Leaner-Instruct, Leaner-Glue for assessing linguistic proficiency, and Leaner-Eval for testing instruction-following ability.\nOur experiments show that leaner pre-training boosts LM learning efficiency. Tiny LMs trained on these datasets outperform those trained on original datasets in instruction-following across different language granularity levels. Moreover, the Leaner-Pretrain dataset's alignment with conventional large LM training sets enables resource-optimized analysis of how learning objectives, model architectures, and training techniques impact performance on language modeling and downstream tasks. Our code and datasets are available at this https URL.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00522.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 35,
    "github_stars": 13,
    "upvote": 2,
    "citing_models": 0,
    "citing_datasets": 3,
    "citing_spaces": 0,
    "citing_collections": 2,
    "citations_by_year": {
        "2025": 2
    },
    "citationCount": 2,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 116,
            "citationCount": 5,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 69,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}