{
    "arxiv_id": "2412.18873",
    "title": "Cross-PCR: A Robust Cross-Source Point Cloud Registration Framework",
    "authors": [
        {
            "name": "G Zhao",
            "citations_all": 67,
            "citations_recent": 67,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "Z Guo",
            "citations_all": 40,
            "citations_recent": 40,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "Z Du",
            "citations_all": 44,
            "citations_recent": 44,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "H Ma",
            "citations_all": 2852,
            "citations_recent": 1575,
            "h_index_all": 24,
            "h_index_recent": 18,
            "i10_index_all": 71,
            "i10_index_recent": 40
        }
    ],
    "abstract": "Due to the density inconsistency and distribution difference between cross-source point clouds, previous methods fail in cross-source point cloud registration. We propose a density-robust feature extraction and matching scheme to achieve robust and accurate cross-source registration. To address the density inconsistency between cross-source data, we introduce a density-robust encoder for extracting density-robust features. To tackle the issue of challenging feature matching and few correct correspondences, we adopt a loose-to-strict matching pipeline with a ``loose generation, strict selection'' idea. Under it, we employ a one-to-many strategy to loosely generate initial correspondences. Subsequently, high-quality correspondences are strictly selected to achieve robust registration through sparse matching and dense matching. On the challenging Kinect-LiDAR scene in the cross-source 3DCSR dataset, our method improves feature matching recall by 63.5 percentage points (pp) and registration recall by 57.6 pp. It also achieves the best performance on 3DMatch, while maintaining robustness under diverse downsampling densities.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.18873.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "keywords": null,
    "num_pages": 9,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 3,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 34,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 62,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}