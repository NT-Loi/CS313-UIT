{
    "arxiv_id": "2501.00031",
    "title": "Distilling Large Language Models for Efficient Clinical Information Extraction",
    "authors": [
        {
            "name": "KS Vedula",
            "citations_all": 27,
            "citations_recent": 27,
            "h_index_all": 2,
            "h_index_recent": 2,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "A Gupta",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "A Swaminathan",
            "citations_all": 942,
            "citations_recent": 928,
            "h_index_all": 14,
            "h_index_recent": 14,
            "i10_index_all": 21,
            "i10_index_recent": 21
        },
        {
            "name": "I Lopez",
            "citations_all": 171,
            "citations_recent": 163,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 6,
            "i10_index_recent": 5
        },
        {
            "name": "S Bedi",
            "citations_all": 430,
            "citations_recent": 429,
            "h_index_all": 9,
            "h_index_recent": 9,
            "i10_index_all": 9,
            "i10_index_recent": 9
        },
        {
            "name": "NH Shah",
            "citations_all": 42196,
            "citations_recent": 27778,
            "h_index_all": 93,
            "h_index_recent": 76,
            "i10_index_all": 331,
            "i10_index_recent": 275
        }
    ],
    "abstract": "Large language models (LLMs) excel at clinical information extraction but their computational demands limit practical deployment. Knowledge distillation--the process of transferring knowledge from larger to smaller models--offers a potential solution. We evaluate the performance of distilled BERT models, which are approximately 1,000 times smaller than modern LLMs, for clinical named entity recognition (NER) tasks. We leveraged state-of-the-art LLMs (Gemini and OpenAI models) and medical ontologies (RxNorm and SNOMED) as teacher labelers for medication, disease, and symptom extraction. We applied our approach to over 3,300 clinical notes spanning five publicly available datasets, comparing distilled BERT models against both their teacher labelers and BERT models fine-tuned on human labels. External validation was conducted using clinical notes from the MedAlign dataset. For disease extraction, F1 scores were 0.82 (teacher model), 0.89 (BioBERT trained on human labels), and 0.84 (BioBERT-distilled). For medication, F1 scores were 0.84 (teacher model), 0.91 (BioBERT-human), and 0.87 (BioBERT-distilled). For symptoms: F1 score of 0.73 (teacher model) and 0.68 (BioBERT-distilled). Distilled BERT models had faster inference (12x, 4x, 8x faster than GPT-4o, o1-mini, and Gemini Flash respectively) and lower costs (85x, 101x, 2x cheaper than GPT-4o, o1-mini, and Gemini Flash respectively). On the external validation dataset, the distilled BERT model achieved F1 scores of 0.883 (medication), 0.726 (disease), and 0.699 (symptom). Distilled BERT models were up to 101x cheaper and 12x faster than state-of-the-art LLMs while achieving similar performance on NER tasks. Distillation offers a computationally efficient and scalable alternative to large LLMs for clinical information extraction.",
    "published_date": "2024-12-21T00:00:00",
    "last_revised_date": "2024-12-21T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00031.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)"
    ],
    "keywords": null,
    "num_pages": 19,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 3
    },
    "citationCount": 3,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 198,
            "citationCount": 7,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 1,
    "embedding": null
}