{
    "arxiv_id": "2412.19218",
    "title": "Transformer-Based Wireless Capsule Endoscopy Bleeding Tissue Detection and Classification",
    "authors": [
        {
            "name": "B Alawode",
            "citations_all": 181,
            "citations_recent": 181,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 6,
            "i10_index_recent": 6
        },
        {
            "name": "S Hamza",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "A Ghimire",
            "citations_all": 59,
            "citations_recent": 59,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 3,
            "i10_index_recent": 3
        },
        {
            "name": "D Velayudhan",
            "citations_all": 285,
            "citations_recent": 281,
            "h_index_all": 9,
            "h_index_recent": 9,
            "i10_index_all": 8,
            "i10_index_recent": 8
        }
    ],
    "abstract": "Informed by the success of the transformer model in various computer vision tasks, we design an end-to-end trainable model for the automatic detection and classification of bleeding and non-bleeding frames extracted from Wireless Capsule Endoscopy (WCE) videos. Based on the DETR model, our model uses the Resnet50 for feature extraction, the transformer encoder-decoder for bleeding and non-bleeding region detection, and a feedforward neural network for classification. Trained in an end-to-end approach on the Auto-WCEBleedGen Version 1 challenge training set, our model performs both detection and classification tasks as a single unit. Our model achieves an accuracy, recall, and F1-score classification percentage score of 98.28, 96.79, and 98.37 respectively, on the Auto-WCEBleedGen version 1 validation set. Further, we record an average precision (AP @ 0.5), mean-average precision (mAP) of 0.7447 and 0.7328 detection results. This earned us a 3rd place position in the challenge. Our code is publicly available via this https URL.",
    "published_date": "2024-12-26T00:00:00",
    "last_revised_date": "2024-12-26T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19218.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "keywords": null,
    "num_pages": 8,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 4
    },
    "citationCount": 4,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 20,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 48,
            "citationCount": 3,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}