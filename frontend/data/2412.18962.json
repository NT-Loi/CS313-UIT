{
    "arxiv_id": "2412.18962",
    "title": "Don't Lose Yourself: Boosting Multimodal Recommendation via Reducing Node-neighbor Discrepancy in Graph Convolutional Network",
    "authors": [
        {
            "name": "Z Chen",
            "citations_all": 223,
            "citations_recent": 223,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 4,
            "i10_index_recent": 4
        },
        {
            "name": "J Xu",
            "citations_all": 3204,
            "citations_recent": 2140,
            "h_index_all": 29,
            "h_index_recent": 24,
            "i10_index_all": 53,
            "i10_index_recent": 47
        },
        {
            "name": "H Hu",
            "citations_all": 5843,
            "citations_recent": 2997,
            "h_index_all": 45,
            "h_index_recent": 29,
            "i10_index_all": 103,
            "i10_index_recent": 88
        }
    ],
    "abstract": "The rapid expansion of multimedia contents has led to the emergence of multimodal recommendation systems. It has attracted increasing attention in recommendation systems because its full utilization of data from different modalities alleviates the persistent data sparsity problem. As such, multimodal recommendation models can learn personalized information about nodes in terms of visual and textual. To further alleviate the data sparsity problem, some previous works have introduced graph convolutional networks (GCNs) for multimodal recommendation systems, to enhance the semantic representation of users and items by capturing the potential relationships between them. However, adopting GCNs inevitably introduces the over-smoothing problem, which make nodes to be too similar. Unfortunately, incorporating multimodal information will exacerbate this challenge because nodes that are too similar will lose the personalized information learned through multimodal information. To address this problem, we propose a novel model that retains the personalized information of ego nodes during feature aggregation by Reducing Node-neighbor Discrepancy (RedN^nD). Extensive experiments on three public datasets show that RedN^nD achieves state-of-the-art performance on accuracy and robustness, with significant improvements over existing GCN-based multimodal frameworks.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.18962.pdf",
    "primary_category": "Information Retrieval (cs.IR)",
    "categories": [
        "Information Retrieval (cs.IR)",
        "Multimedia (cs.MM)"
    ],
    "keywords": null,
    "num_pages": 5,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 7,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 1,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 33,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}