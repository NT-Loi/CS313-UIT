{
    "arxiv_id": "2412.18487",
    "title": "Segment-Based Attention Masking for GPTs",
    "authors": [
        {
            "name": "S Katz",
            "citations_all": 57,
            "citations_recent": 57,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "L Ringel",
            "citations_all": 68,
            "citations_recent": 68,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "Y Romano",
            "citations_all": 5985,
            "citations_recent": 5117,
            "h_index_all": 31,
            "h_index_recent": 30,
            "i10_index_all": 45,
            "i10_index_recent": 43
        },
        {
            "name": "L Wolf",
            "citations_all": 44756,
            "citations_recent": 24812,
            "h_index_all": 85,
            "h_index_recent": 70,
            "i10_index_all": 283,
            "i10_index_recent": 241
        }
    ],
    "abstract": "Modern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial \"prefill\" phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved.",
    "published_date": "2024-12-24T00:00:00",
    "last_revised_date": "2024-12-24T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.18487.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)"
    ],
    "keywords": null,
    "num_pages": 14,
    "github_stars": 7,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 1
    },
    "citationCount": 1,
    "venue": {
        "name": "Annual Meeting of the Association for Computational Linguistics",
        "type": "conference"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 16,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 86,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 26,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 9328,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 77,
            "citationCount": 1385,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 59,
            "citationCount": 584,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 131,
            "citationCount": 13793,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 55,
            "citationCount": 3283,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 146,
            "citationCount": 47424,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 12,
            "citationCount": 1261,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 40,
            "citationCount": 2233,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 134,
            "citationCount": 22102,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 59,
            "citationCount": 1595,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 1846,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 22,
            "citationCount": 3079,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 53,
            "citationCount": 1839,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 3300,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 41,
            "citationCount": 145209,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 100793,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 42,
            "citationCount": 2330,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 73,
            "citationCount": 12929,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}