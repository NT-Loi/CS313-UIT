{
    "arxiv_id": "2412.19829",
    "title": "GFormer: Accelerating Large Language Models with Optimized Transformers on Gaudi Processors",
    "authors": [
        {
            "name": "C Zhang",
            "citations_all": 641,
            "citations_recent": 640,
            "h_index_all": 13,
            "h_index_recent": 13,
            "i10_index_all": 16,
            "i10_index_recent": 16
        },
        {
            "name": "X Ding",
            "citations_all": 2,
            "citations_recent": 2,
            "h_index_all": 1,
            "h_index_recent": 1,
            "i10_index_all": 0,
            "i10_index_recent": 0
        },
        {
            "name": "B Sun",
            "citations_all": 35,
            "citations_recent": 35,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "X Yu",
            "citations_all": 865,
            "citations_recent": 669,
            "h_index_all": 16,
            "h_index_recent": 15,
            "i10_index_all": 29,
            "i10_index_recent": 24
        },
        {
            "name": "W Zheng",
            "citations_all": 322,
            "citations_recent": 314,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 7,
            "i10_index_recent": 6
        },
        {
            "name": "Z Xie",
            "citations_all": 677,
            "citations_recent": 670,
            "h_index_all": 13,
            "h_index_recent": 13,
            "i10_index_all": 16,
            "i10_index_recent": 16
        },
        {
            "name": "D Tao",
            "citations_all": 3887,
            "citations_recent": 3523,
            "h_index_all": 35,
            "h_index_recent": 34,
            "i10_index_all": 70,
            "i10_index_recent": 66
        }
    ],
    "abstract": "Heterogeneous hardware like Gaudi processor has been developed to enhance computations, especially matrix operations for Transformer-based large language models (LLMs) for generative AI tasks. However, our analysis indicates that Transformers are not fully optimized on such emerging hardware, primarily due to inadequate optimizations in non-matrix computational kernels like Softmax and in heterogeneous resource utilization, particularly when processing long sequences. To address these issues, we propose an integrated approach (called GFormer) that merges sparse and linear attention mechanisms. GFormer aims to maximize the computational capabilities of the Gaudi processor's Matrix Multiplication Engine (MME) and Tensor Processing Cores (TPC) without compromising model quality. GFormer includes a windowed self-attention kernel and an efficient outer product kernel for causal linear attention, aiming to optimize LLM inference on Gaudi processors. Evaluation shows that GFormer significantly improves efficiency and model performance across various tasks on the Gaudi processor and outperforms state-of-the-art GPUs.",
    "published_date": "2024-12-19T00:00:00",
    "last_revised_date": "2024-12-19T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19829.pdf",
    "primary_category": "Hardware Architecture (cs.AR)",
    "categories": [
        "Hardware Architecture (cs.AR)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 12,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}