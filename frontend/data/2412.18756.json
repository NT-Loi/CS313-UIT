{
    "arxiv_id": "2412.18756",
    "title": "Towards a Statistical Understanding of Neural Networks: Beyond the Neural Tangent Kernel Theories",
    "authors": [
        {
            "name": "H Zhang",
            "citations_all": 158,
            "citations_recent": 158,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 6,
            "i10_index_recent": 6
        },
        {
            "name": "J Lai",
            "citations_all": 54,
            "citations_recent": 54,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "Y Li",
            "citations_all": 197,
            "citations_recent": 197,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 8,
            "i10_index_recent": 8
        },
        {
            "name": "Q Lin",
            "citations_all": 615,
            "citations_recent": 541,
            "h_index_all": 12,
            "h_index_recent": 11,
            "i10_index_all": 16,
            "i10_index_recent": 14
        },
        {
            "name": "JS Liu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        }
    ],
    "abstract": "A primary advantage of neural networks lies in their feature learning characteristics, which is challenging to theoretically analyze due to the complexity of their training dynamics. We propose a new paradigm for studying feature learning and the resulting benefits in generalizability. After reviewing the neural tangent kernel (NTK) theory and recent results in kernel regression, which address the generalization issue of sufficiently wide neural networks, we examine limitations and implications of the fixed kernel theory (as the NTK theory) and review recent theoretical advancements in feature learning. Moving beyond the fixed kernel/feature theory, we consider neural networks as adaptive feature models. Finally, we propose an over-parameterized Gaussian sequence model as a prototype model to study the feature learning characteristics of neural networks.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.18756.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Statistics Theory (math.ST)"
    ],
    "keywords": null,
    "num_pages": 25,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 2
    },
    "citationCount": 2,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}