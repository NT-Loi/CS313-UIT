{
    "arxiv_id": "2412.20382",
    "title": "Natural Language Fine-Tuning",
    "authors": [
        {
            "name": "J Liu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "Y Wang",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "Z Lin",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "M Chen",
            "citations_all": 52801,
            "citations_recent": 31785,
            "h_index_all": 102,
            "h_index_recent": 72,
            "i10_index_all": 366,
            "i10_index_recent": 265
        },
        {
            "name": "Y Hao",
            "citations_all": 8362,
            "citations_recent": 6479,
            "h_index_all": 35,
            "h_index_recent": 33,
            "i10_index_all": 63,
            "i10_index_recent": 61
        },
        {
            "name": "L Hu",
            "citations_all": 4533,
            "citations_recent": 3527,
            "h_index_all": 35,
            "h_index_recent": 32,
            "i10_index_all": 71,
            "i10_index_recent": 69
        }
    ],
    "abstract": "Large language model fine-tuning techniques typically depend on extensive labeled data, external guidance, and feedback, such as human alignment, scalar rewards, and demonstration. However, in practical application, the scarcity of specific knowledge poses unprecedented challenges to existing fine-tuning techniques. In this paper, focusing on fine-tuning tasks in specific domains with limited data, we introduce Natural Language Fine-Tuning (NLFT), which utilizes natural language for fine-tuning for the first time. By leveraging the strong language comprehension capability of the target LM, NLFT attaches the guidance of natural language to the token-level outputs. Then, saliency tokens are identified with calculated probabilities. Since linguistic information is effectively utilized in NLFT, our proposed method significantly reduces training costs. It markedly enhances training efficiency, comprehensively outperforming reinforcement fine-tuning algorithms in accuracy, time-saving, and resource conservation. Additionally, on the macro level, NLFT can be viewed as a token-level fine-grained optimization of SFT, thereby efficiently replacing the SFT process without the need for warm-up (as opposed to ReFT requiring multiple rounds of warm-up with SFT). Compared to SFT, NLFT does not increase the algorithmic complexity, maintaining O(n). Extensive experiments on the GSM8K dataset demonstrate that NLFT, with only 50 data instances, achieves an accuracy increase that exceeds SFT by 219%. Compared to ReFT, the time complexity and space complexity of NLFT are reduced by 78.27% and 92.24%, respectively. The superior technique of NLFT is paving the way for the deployment of various innovative LLM fine-tuning applications when resources are limited at network edges.\nOur code has been released at this https URL.",
    "published_date": "2024-12-29T00:00:00",
    "last_revised_date": "2024-12-29T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20382.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 11,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 4
    },
    "citationCount": 4,
    "venue": {
        "name": "arXiv.org",
        "type": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 112,
            "citationCount": 11,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 19,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 64,
            "citationCount": 203,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 64,
            "citationCount": 196,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 69,
            "citationCount": 15,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 252,
            "citationCount": 205,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 92,
            "citationCount": 2460,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 60,
            "citationCount": 5704,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 69,
            "citationCount": 320,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 80,
            "citationCount": 4761,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 83,
            "citationCount": 15631,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 31,
            "citationCount": 5987,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 65,
            "citationCount": 13156,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 74,
            "citationCount": 275,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 8,
            "citationCount": 89,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 35,
            "citationCount": 26395,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 14,
            "citationCount": 21945,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}