{
    "arxiv_id": "2112.13953",
    "title": "Source Feature Compression for Object Classification in Vision-Based Underwater Robotics",
    "authors": [
        {
            "name": "X Zhao",
            "citations_all": 295,
            "citations_recent": 182,
            "h_index_all": 12,
            "h_index_recent": 9,
            "i10_index_all": 14,
            "i10_index_recent": 8
        },
        {
            "name": "M Rahmati",
            "citations_all": 564,
            "citations_recent": 465,
            "h_index_all": 14,
            "h_index_recent": 12,
            "i10_index_all": 19,
            "i10_index_recent": 15
        },
        {
            "name": "D Pompili",
            "citations_all": 18219,
            "citations_recent": 7287,
            "h_index_all": 54,
            "h_index_recent": 33,
            "i10_index_all": 171,
            "i10_index_recent": 115
        }
    ],
    "abstract": "New efficient source feature compression solutions are proposed based on a two-stage Walsh-Hadamard Transform (WHT) for Convolutional Neural Network (CNN)-based object classification in underwater robotics. The object images are firstly transformed by WHT following a two-stage process. The transform-domain tensors have large values concentrated in the upper left corner of the matrices in the RGB channels. By observing this property, the transform-domain matrix is partitioned into inner and outer regions. Consequently, two novel partitioning methods are proposed in this work: (i) fixing the size of inner and outer regions; and (ii) adjusting the size of inner and outer regions adaptively per image. The proposals are evaluated with an underwater object dataset captured from the Raritan River in New Jersey, USA. It is demonstrated and verified that the proposals reduce the training time effectively for learning-based underwater object classification task and increase the accuracy compared with the competing methods. The object classification is an essential part of a vision-based underwater robot that can sense the environment and navigate autonomously. Therefore, the proposed method is well-suited for efficient computer vision-based tasks in underwater robotics applications.",
    "published_date": "2021-12-28T00:00:00",
    "last_revised_date": "2021-12-28T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2112.13953.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Robotics (cs.RO)"
    ],
    "keywords": null,
    "num_pages": 9,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 17,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 9,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 17,
            "citationCount": 2,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 8,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 19,
            "citationCount": 14,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 32,
            "citationCount": 20,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 287,
            "citationCount": 959,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 43,
            "citationCount": 1014,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 46,
            "citationCount": 51,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 38,
            "citationCount": 23051,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 27,
            "citationCount": 16329,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 53,
            "citationCount": 212885,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 210,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 263,
            "citationCount": 45783,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 43,
            "citationCount": 107060,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 71,
            "citationCount": 124714,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 356,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 71,
            "citationCount": 58,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}