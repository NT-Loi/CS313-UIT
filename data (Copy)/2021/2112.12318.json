{
    "arxiv_id": "2112.12318",
    "title": "Investigating Effect of Dialogue History in Multilingual Task Oriented Dialogue Systems",
    "authors": [
        {
            "name": "M Sun",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "K Huang",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "M Moradshahi",
            "citations_all": 1413,
            "citations_recent": 1406,
            "h_index_all": 9,
            "h_index_recent": 9,
            "i10_index_all": 8,
            "i10_index_recent": 8
        }
    ],
    "abstract": "While the English virtual assistants have achieved exciting performance with an enormous amount of training resources, the needs of non-English-speakers have not been satisfied well. Up to Dec 2021, Alexa, one of the most popular smart speakers around the world, is able to support 9 different languages [1], while there are thousands of languages in the world, 91 of which are spoken by more than 10 million people according to statistics published in 2019 [2]. However, training a virtual assistant in other languages than English is often more difficult, especially for those low-resource languages. The lack of high-quality training data restricts the performance of models, resulting in poor user satisfaction. Therefore, we devise an efficient and effective training solution for multilingual task-orientated dialogue systems, using the same dataset generation pipeline and end-to-end dialogue system architecture as BiToD[5], which adopted some key design choices for a minimalistic natural language design where formal dialogue states are used in place of natural language inputs. This reduces the room for error brought by weaker natural language models, and ensures the model can correctly extract the essential slot values needed to perform dialogue state tracking (DST). Our goal is to reduce the amount of natural language encoded at each turn, and the key parameter we investigate is the number of turns (H) to feed as history to model. We first explore the turning point where increasing H begins to yield limiting returns on the overall performance. Then we examine whether the examples a model with small H gets wrong can be categorized in a way for the model to do few-shot finetuning on. Lastly, will explore the limitations of this approach, and whether there is a certain type of examples that this approach will not be able to resolve.",
    "published_date": "2021-12-23T00:00:00",
    "last_revised_date": "2021-12-23T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2112.12318.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 9,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2024": 1
    },
    "citationCount": 1,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 6,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 77,
            "citationCount": 63,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 77,
            "citationCount": 72,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 66,
            "citationCount": 1897,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 23,
            "citationCount": 371,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}