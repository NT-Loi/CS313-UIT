{
    "arxiv_id": "2312.16542",
    "title": "FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient GNNs",
    "authors": [
        {
            "name": "C Adnel",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "I Rekik",
            "citations_all": 4940,
            "citations_recent": 4320,
            "h_index_all": 36,
            "h_index_recent": 33,
            "i10_index_all": 97,
            "i10_index_recent": 92
        }
    ],
    "abstract": "Graph Neural Network (GNN) ushered in a new era of machine learning with interconnected datasets. While traditional neural networks can only be trained on independent samples, GNN allows for the inclusion of inter-sample interactions in the training process. This gain, however, incurs additional memory cost, rendering most GNNs unscalable for real-world applications involving vast and complicated networks with tens of millions of nodes (e.g., social circles, web graphs, and brain graphs). This means that storing the graph in the main memory can be difficult, let alone training the GNN model with significantly less GPU memory. While much of the recent literature has focused on either mini-batching GNN methods or quantization, graph reduction methods remain largely scarce. Furthermore, present graph reduction approaches have several drawbacks. First, most graph reduction focuses only on the inference stage (e.g., condensation and distillation) and requires full graph GNN training, which does not reduce training memory footprint. Second, many methods focus solely on the graph's structural aspect, ignoring the initial population feature-label distribution, resulting in a skewed post-reduction label distribution. Here, we propose a Feature-Label COnstrained graph Net collapse, FALCON, to address these limitations. Our three core contributions lie in (i) designing FALCON, a topology-aware graph reduction technique that preserves feature-label distribution; (ii) implementation of FALCON with other memory reduction methods (i.e., mini-batched GNN and quantization) for further memory reduction; (iii) extensive benchmarking and ablation studies against SOTA methods to evaluate FALCON memory reduction. Our extensive results show that FALCON can significantly collapse various public datasets while achieving equal prediction quality across GNN models. Code: this https URL",
    "published_date": "2023-12-27T00:00:00",
    "last_revised_date": "2023-12-27T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2312.16542.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 11,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 1
    },
    "citationCount": 1,
    "venue": {
        "name": "IEEE Transactions on Neural Networks and Learning Systems",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 34,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 48,
            "citationCount": 5,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 59,
            "citationCount": 23,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 52,
            "citationCount": 83,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 35,
            "citationCount": 40,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 27,
            "citationCount": 7,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 141,
            "citationCount": 41,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 66,
            "citationCount": 122,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 41,
            "citationCount": 863,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 61,
            "citationCount": 173,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 254,
            "citationCount": 547,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 62,
            "citationCount": 136,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 119,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 127,
            "citationCount": 218,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 54,
            "citationCount": 82,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 13,
            "citationCount": 92,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 81,
            "citationCount": 418,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 35,
            "citationCount": 1012,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 17,
            "citationCount": 1380,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 3364,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 14,
            "citationCount": 75,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 45,
            "citationCount": 21749,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 58,
            "citationCount": 555,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 38,
            "citationCount": 30784,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 2225,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 60,
            "citationCount": 182,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 27,
            "citationCount": 4581,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 17,
            "citationCount": 16069,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 6113,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 2,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 15,
            "citationCount": 4,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 66,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 52,
            "citationCount": 16,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 26,
            "citationCount": 231,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}