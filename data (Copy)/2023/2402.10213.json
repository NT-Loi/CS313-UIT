{
    "arxiv_id": "2402.10213",
    "title": "Clustering Inductive Biases with Unrolled Networks",
    "authors": [
        {
            "name": "J Huml",
            "citations_all": 25,
            "citations_recent": 25,
            "h_index_all": 2,
            "h_index_recent": 2,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "A Tasissa",
            "citations_all": 277,
            "citations_recent": 261,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 6,
            "i10_index_recent": 6
        },
        {
            "name": "D Ba",
            "citations_all": 2855,
            "citations_recent": 1955,
            "h_index_all": 23,
            "h_index_recent": 20,
            "i10_index_all": 37,
            "i10_index_recent": 30
        }
    ],
    "abstract": "The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \\emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitly, locally organized for spectral clustering through a Laplacian quadratic form of a bipartite graph, which generates a diverse set of artificial receptive fields that match primate data in V1 as faithfully as recent contrastive frameworks like Local Low Dimensionality, or LLD \\citep{lld} that discard sparse dictionary learning. By unifying sparse and smooth coding in models of the early visual cortex through our autoencoder, we also show that our regularization can be interpreted as early-stage specialization of receptive fields to certain classes of stimuli; that is, we induce a weak clustering bias for later stages of cortex where functional and spatial segregation (i.e. topography) are known to occur. The results show an imperative for \\emph{spatial regularization} of both the receptive fields and firing rates to begin to describe feature disentanglement in V1 and beyond.",
    "published_date": "2023-11-30T00:00:00",
    "last_revised_date": "2023-11-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2402.10213.pdf",
    "primary_category": "Neurons and Cognition (q-bio.NC)",
    "categories": [
        "Neurons and Cognition (q-bio.NC)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 2,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 3,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 110,
            "citationCount": 1115,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 33,
            "citationCount": 421,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}