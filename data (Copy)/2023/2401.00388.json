{
    "arxiv_id": "2401.00388",
    "title": "FusionMind -- Improving question and answering with external context fusion",
    "authors": [
        {
            "name": "S Verma",
            "citations_all": 287,
            "citations_recent": 287,
            "h_index_all": 5,
            "h_index_recent": 5,
            "i10_index_all": 5,
            "i10_index_recent": 5
        },
        {
            "name": "M Parmar",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "P Choudhary",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "S Porwal",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        }
    ],
    "abstract": "Answering questions using pre-trained language models (LMs) and knowledge graphs (KGs) presents challenges in identifying relevant knowledge and performing joint this http URL compared LMs (fine-tuned for the task) with the previously published QAGNN method for the Question-answering (QA) objective and further measured the impact of additional factual context on the QAGNN performance. The QAGNN method employs LMs to encode QA context and estimate KG node importance, and effectively update the question choice entity representations using Graph Neural Networks (GNNs). We further experimented with enhancing the QA context encoding by incorporating relevant knowledge facts for the question stem. The models are trained on the OpenbookQA dataset, which contains ~6000 4-way multiple choice questions and is widely used as a benchmark for QA tasks. Through our experimentation, we found that incorporating knowledge facts context led to a significant improvement in performance. In contrast, the addition of knowledge graphs to language models resulted in only a modest increase. This suggests that the integration of contextual knowledge facts may be more impactful for enhancing question answering performance compared to solely adding knowledge graphs.",
    "published_date": "2023-12-31T00:00:00",
    "last_revised_date": "2023-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2401.00388.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)"
    ],
    "keywords": null,
    "num_pages": 5,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 3
    },
    "citationCount": 3,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 38,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 16,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 30,
            "citationCount": 3,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 119,
            "citationCount": 6,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 54,
            "citationCount": 27,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 76,
            "citationCount": 27,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 137,
            "citationCount": 11,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 65,
            "citationCount": 659,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 163,
            "citationCount": 271,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 22,
            "citationCount": 25,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 68,
            "citationCount": 25931,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 53,
            "citationCount": 1829,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 8571,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}