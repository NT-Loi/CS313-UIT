{
    "arxiv_id": "2412.20891",
    "title": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models",
    "authors": [
        {
            "name": "X Hu",
            "citations_all": 216,
            "citations_recent": 213,
            "h_index_all": 6,
            "h_index_recent": 6,
            "i10_index_all": 5,
            "i10_index_recent": 5
        },
        {
            "name": "X Cheng",
            "citations_all": 3,
            "citations_recent": 3,
            "h_index_all": 1,
            "h_index_recent": 1,
            "i10_index_all": 0,
            "i10_index_recent": 0
        },
        {
            "name": "P Liu",
            "citations_all": 7658,
            "citations_recent": 7644,
            "h_index_all": 10,
            "h_index_recent": 10,
            "i10_index_all": 10,
            "i10_index_recent": 10
        },
        {
            "name": "W Liu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "J Luan",
            "citations_all": 1528,
            "citations_recent": 1443,
            "h_index_all": 19,
            "h_index_recent": 19,
            "i10_index_all": 32,
            "i10_index_recent": 29
        },
        {
            "name": "B Wang",
            "citations_all": 12992,
            "citations_recent": 9702,
            "h_index_all": 50,
            "h_index_recent": 45,
            "i10_index_all": 141,
            "i10_index_recent": 90
        },
        {
            "name": "Y Liu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        }
    ],
    "abstract": "Low-rank adaptation (LoRA) reduces the computational and memory demands of fine-tuning large language models (LLMs) by approximating updates with low-rank matrices. However, low-rank approximation in two-dimensional space fails to capture high-dimensional structures within the target matrix. Recently, tensor decomposition methods have been explored for fine-tuning LLMs, leveraging their ability to extract structured information. Yet, these approaches primarily rely on random initialization, and the impact of initialization on tensor adaptation remains underexplored. In this paper, we reveal that random initialization significantly diverges from the validation loss achieved by full fine-tuning. To address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which leverages the Matrix Product Operator (MPO) decomposition of pre-trained weights for effective initialization in fine-tuning LLMs. Additionally, we introduce QDoTA, a quantized version of DoTA designed for 4-bit quantization. Experiments on commonsense and arithmetic reasoning tasks show that DoTA outperforms random initialization methods with fewer parameters. QDoTA further reduces memory consumption and achieves comparable performance to DoTA on commonsense reasoning tasks. We will release our code to support future research.",
    "published_date": "2024-12-30T00:00:00",
    "last_revised_date": "2024-12-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20891.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": [
        "Large Language Models Parameter Efficient Fine-Tuning Tensor Decomposition Matrix Product Operator Initialization"
    ],
    "num_pages": 12,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 1,
    "citations_by_year": {},
    "citationCount": 2,
    "venue": {
        "name": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
        "type": "conference",
        "ranking": "B"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 29,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 51,
            "citationCount": 5,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 290,
            "citationCount": 32,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 30,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 43,
            "citationCount": 18,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 9419,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 45,
            "citationCount": 69,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 74,
            "citationCount": 44,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 56,
            "citationCount": 22,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 16,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 89,
            "citationCount": 4,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 73,
            "citationCount": 164,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 38,
            "citationCount": 267,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 60,
            "citationCount": 13,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 59,
            "citationCount": 590,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 30,
            "citationCount": 26,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 19,
            "citationCount": 141,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 84,
            "citationCount": 494,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 131,
            "citationCount": 13838,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 73,
            "citationCount": 3184,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 57,
            "citationCount": 344,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 185,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 31,
            "citationCount": 5987,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 41,
            "citationCount": 1033,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 2,
            "citationCount": 5098,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 65,
            "citationCount": 13156,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 110,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 68,
            "citationCount": 41,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 451,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 17,
            "citationCount": 329,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 94,
            "citationCount": 944,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 2,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 101021,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 75,
            "citationCount": 25008,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}