{
    "arxiv_id": "1712.06180",
    "title": "Towards a Deep Reinforcement Learning Approach for Tower Line Wars",
    "authors": [
        {
            "name": "PA Andersen",
            "citations_all": 469,
            "citations_recent": 417,
            "h_index_all": 10,
            "h_index_recent": 9,
            "i10_index_all": 10,
            "i10_index_recent": 9
        },
        {
            "name": "M Goodwin",
            "citations_all": 4217,
            "citations_recent": 3499,
            "h_index_all": 32,
            "h_index_recent": 29,
            "i10_index_all": 81,
            "i10_index_recent": 68
        },
        {
            "name": "OC Granmo",
            "citations_all": 5084,
            "citations_recent": 3668,
            "h_index_all": 36,
            "h_index_recent": 30,
            "i10_index_all": 117,
            "i10_index_recent": 79
        }
    ],
    "abstract": "There have been numerous breakthroughs with reinforcement learning in the recent years, perhaps most notably on Deep Reinforcement Learning successfully playing and winning relatively advanced computer games. There is undoubtedly an anticipation that Deep Reinforcement Learning will play a major role when the first AI masters the complicated game plays needed to beat a professional Real-Time Strategy game player. For this to be possible, there needs to be a game environment that targets and fosters AI research, and specifically Deep Reinforcement Learning. Some game environments already exist, however, these are either overly simplistic such as Atari 2600 or complex such as Starcraft II from Blizzard Entertainment. We propose a game environment in between Atari 2600 and Starcraft II, particularly targeting Deep Reinforcement Learning algorithm research. The environment is a variant of Tower Line Wars from Warcraft III, Blizzard Entertainment. Further, as a proof of concept that the environment can harbor Deep Reinforcement algorithms, we propose and apply a Deep Q-Reinforcement architecture. The architecture simplifies the state space so that it is applicable to Q-learning, and in turn improves performance compared to current state-of-the-art methods. Our experiments show that the proposed architecture can learn to play the environment well, and score 33% better than standard Deep Q-learning which in turn proves the usefulness of the game environment.",
    "published_date": "2017-12-17T00:00:00",
    "last_revised_date": "2017-12-17T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/1712.06180.pdf",
    "primary_category": "Artificial Intelligence (cs.AI)",
    "categories": [
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 14,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2018": 3,
        "2019": 3,
        "2020": 5,
        "2021": 1,
        "2024": 2,
        "2025": 1
    },
    "citationCount": 15,
    "venue": {
        "name": "Lecture Notes in Computer Science",
        "type": "journal",
        "ranking": "Q1"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 22,
            "citationCount": 4,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 9,
            "citationCount": 8,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 80,
            "citationCount": 16,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 13,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 38,
            "citationCount": 912,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 32,
            "citationCount": 263,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 34,
            "citationCount": 905,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 28,
            "citationCount": 3954,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 43,
            "citationCount": 3963,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 29,
            "citationCount": 8156,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 35,
            "citationCount": 13944,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 13,
            "citationCount": 43,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 30,
            "citationCount": 12761,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 3137,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 141,
            "citationCount": 339,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}