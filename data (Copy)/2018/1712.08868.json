{
    "arxiv_id": "1712.08868",
    "title": "Use of Generative Adversarial Network for Cross-Domain Change Detection",
    "authors": [
        {
            "name": "Y Kousuke",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "T Kanji",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "S Takuma",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        }
    ],
    "abstract": "This paper addresses the problem of cross-domain change detection from a novel perspective of image-to-image translation. In general, change detection aims to identify interesting changes between a given query image and a reference image of the same scene taken at a different time. This problem becomes a challenging one when query and reference images involve different domains (e.g., time of the day, weather, and season) due to variations in object appearance and a limited amount of training examples. In this study, we address the above issue by leveraging a generative adversarial network (GAN). Our key concept is to use a limited amount of training data to train a GAN-based image translator that maps a reference image to a virtual image that cannot be discriminated from query domain images. This enables us to treat the cross-domain change detection task as an in-domain image comparison. This allows us to leverage the large body of literature on in-domain generic change detectors. In addition, we also consider the use of visual place recognition as a method for mining more appropriate reference images over the space of virtual images. Experiments validate efficacy of the proposed approach.",
    "published_date": "2017-12-24T00:00:00",
    "last_revised_date": "2017-12-24T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/1712.08868.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "keywords": null,
    "num_pages": 6,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2020": 1,
        "2021": 1,
        "2023": 1,
        "2024": 1
    },
    "citationCount": 4,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 53,
            "citationCount": 33,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 165,
            "citationCount": 23,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 16,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 13,
            "citationCount": 9,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 70,
            "citationCount": 21053,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 534,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 88,
            "citationCount": 330,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 15,
            "citationCount": 11,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 32,
            "citationCount": 29,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 108,
            "citationCount": 864,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 17,
            "citationCount": 80,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 33,
            "citationCount": 33,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 53,
            "citationCount": 109,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 16,
            "citationCount": 34,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 12,
            "citationCount": 36,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 130,
            "citationCount": 2114,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 48775,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 24,
            "citationCount": 37701,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 15435,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}