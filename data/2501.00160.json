{
    "arxiv_id": "2501.00160",
    "title": "Deterministic Model of Incremental Multi-Agent Boltzmann Q-Learning: Transient Cooperation, Metastability, and Oscillations",
    "authors": [
        {
            "name": "D Goll",
            "citations_all": 28,
            "citations_recent": 28,
            "h_index_all": 1,
            "h_index_recent": 1,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "J Heitzig",
            "citations_all": 5454,
            "citations_recent": 3126,
            "h_index_all": 36,
            "h_index_recent": 29,
            "i10_index_all": 60,
            "i10_index_recent": 50
        },
        {
            "name": "W Barfuss",
            "citations_all": 1323,
            "citations_recent": 1224,
            "h_index_all": 18,
            "h_index_recent": 17,
            "i10_index_all": 22,
            "i10_index_recent": 20
        }
    ],
    "abstract": "Multi-Agent Reinforcement Learning involves agents that learn together in a shared environment, leading to emergent dynamics sensitive to initial conditions and parameter variations. A Dynamical Systems approach, which studies the evolution of multi-component systems over time, has uncovered some of the underlying dynamics by constructing deterministic approximation models of stochastic algorithms. In this work, we demonstrate that even in the simplest case of independent Q-learning with a Boltzmann exploration policy, significant discrepancies arise between the actual algorithm and previous approximations. We elaborate why these models actually approximate interesting variants rather than the original incremental algorithm. To explain the discrepancies, we introduce a new discrete-time approximation model that explicitly accounts for agents' update frequencies within the learning process and show that its dynamics fundamentally differ from the simplified dynamics of prior models. We illustrate the usefulness of our approach by applying it to the question of spontaneous cooperation in social dilemmas, specifically the Prisoner's Dilemma as the simplest case study. We identify conditions under which the learning behaviour appears as long-term stable cooperation from an external perspective. However, our model shows that this behaviour is merely a metastable transient phase and not a true equilibrium, making it exploitable. We further exemplify how specific parameter settings can significantly exacerbate the moving target problem in independent learning. Through a systematic analysis of our model, we show that increasing the discount factor induces oscillations, preventing convergence to a joint policy. These oscillations arise from a supercritical Neimark-Sacker bifurcation, which transforms the unique stable fixed point into an unstable focus surrounded by a stable limit cycle.",
    "published_date": "2024-12-30T00:00:00",
    "last_revised_date": "2024-12-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00160.pdf",
    "primary_category": "Multiagent Systems (cs.MA)",
    "categories": [
        "Multiagent Systems (cs.MA)",
        "Adaptation and Self-Organizing Systems (nlin.AO)",
        "Physics and Society (physics.soc-ph)"
    ],
    "keywords": null,
    "num_pages": 20,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}