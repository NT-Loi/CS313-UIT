{
    "arxiv_id": "2412.19821",
    "title": "Nanoscaling Floating-Point (NxFP): NanoMantissa, Adaptive Microexponents, and Code Recycling for Direct-Cast Compression of Large Language Models",
    "authors": [
        {
            "name": "YC Lo",
            "citations_all": 282,
            "citations_recent": 276,
            "h_index_all": 7,
            "h_index_recent": 7,
            "i10_index_all": 4,
            "i10_index_recent": 4
        },
        {
            "name": "GY Wei",
            "citations_all": 17701,
            "citations_recent": 9159,
            "h_index_all": 65,
            "h_index_recent": 42,
            "i10_index_all": 194,
            "i10_index_recent": 137
        },
        {
            "name": "D Brooks",
            "citations_all": 28393,
            "citations_recent": 12672,
            "h_index_all": 71,
            "h_index_recent": 47,
            "i10_index_all": 217,
            "i10_index_recent": 149
        }
    ],
    "abstract": "As cutting-edge large language models (LLMs) continue to transform various industries, their fast-growing model size and sequence length have led to memory traffic and capacity challenges. Recently, AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm have proposed a Microscaling standard (Mx), which augments block floating-point with microexponents to achieve promising perplexity-to-footprint trade-offs. However, the Microscaling suffers from significant perplexity degradation on modern LLMs with less than six bits. This paper profiles modern LLMs and identifies three main challenges of low-bit Microscaling format, i.e., inaccurate tracking of outliers, vacant quantization levels, and wasted binary code. In response, Nanoscaling (NxFP) proposes three techniques, i.e., NanoMantissa, Adaptive Microexponent, and Code Recycling to enable better accuracy and smaller memory footprint than state-of-the-art MxFP. Experimental results on direct-cast inference across various modern LLMs demonstrate that our proposed methods outperform state-of-the-art MxFP by up to 0.64 in perplexity and by up to 30% in accuracy on MMLU benchmarks. Furthermore, NxFP reduces memory footprint by up to 16% while achieving comparable perplexity as MxFP.",
    "published_date": "2024-12-15T00:00:00",
    "last_revised_date": "2024-12-15T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19821.pdf",
    "primary_category": "Hardware Architecture (cs.AR)",
    "categories": [
        "Hardware Architecture (cs.AR)",
        "Artificial Intelligence (cs.AI)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 12,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 2
    },
    "citationCount": 2,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 52,
            "citationCount": 2,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 53,
            "citationCount": 1,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 1,
    "embedding": null
}