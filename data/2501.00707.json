{
    "arxiv_id": "2501.00707",
    "title": "Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability",
    "authors": [
        {
            "name": "H Zeng",
            "citations_all": 492,
            "citations_recent": 357,
            "h_index_all": 12,
            "h_index_recent": 9,
            "i10_index_all": 14,
            "i10_index_recent": 9
        },
        {
            "name": "S Cui",
            "citations_all": 113,
            "citations_recent": 112,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 4,
            "i10_index_recent": 4
        },
        {
            "name": "B Chen",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "A Peng",
            "citations_all": 726,
            "citations_recent": 427,
            "h_index_all": 12,
            "h_index_recent": 10,
            "i10_index_all": 13,
            "i10_index_recent": 10
        }
    ],
    "abstract": "Adversarial examples' (AE) transferability refers to the phenomenon that AEs crafted with one surrogate model can also fool other models. Notwithstanding remarkable progress in untargeted transferability, its targeted counterpart remains challenging. This paper proposes an everywhere scheme to boost targeted transferability. Our idea is to attack a victim image both globally and locally. We aim to optimize 'an army of targets' in every local image region instead of the previous works that optimize a high-confidence target in the image. Specifically, we split a victim image into non-overlap blocks and jointly mount a targeted attack on each block. Such a strategy mitigates transfer failures caused by attention inconsistency between surrogate and victim models and thus results in stronger transferability. Our approach is method-agnostic, which means it can be easily combined with existing transferable attacks for even higher transferability. Extensive experiments on ImageNet demonstrate that the proposed approach universally improves the state-of-the-art targeted attacks by a clear margin, e.g., the transferability of the widely adopted Logit attack can be improved by 28.8%-300%.We also evaluate the crafted AEs on a real-world platform: Google Cloud Vision. Results further support the superiority of the proposed method.",
    "published_date": "2025-01-01T00:00:00",
    "last_revised_date": "2025-01-01T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00707.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)",
        "Cryptography and Security (cs.CR)"
    ],
    "keywords": null,
    "num_pages": 11,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "ranking": "A"
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}