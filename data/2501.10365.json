{
    "arxiv_id": "2501.10365",
    "title": "Can LLMs Identify Gaps and Misconceptions in Students' Code Explanations?",
    "authors": [
        {
            "name": "P Oli",
            "citations_all": 2211,
            "citations_recent": 2209,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 8,
            "i10_index_recent": 8
        },
        {
            "name": "R Banjade",
            "citations_all": 2202,
            "citations_recent": 2200,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 8,
            "i10_index_recent": 8
        },
        {
            "name": "AM Olney",
            "citations_all": 7348,
            "citations_recent": 3088,
            "h_index_all": 38,
            "h_index_recent": 26,
            "i10_index_all": 79,
            "i10_index_recent": 48
        },
        {
            "name": "V Rus",
            "citations_all": 7162,
            "citations_recent": 2756,
            "h_index_all": 43,
            "h_index_recent": 24,
            "i10_index_all": 133,
            "i10_index_recent": 75
        }
    ],
    "abstract": "This paper investigates various approaches using Large Language Models (LLMs) to identify gaps and misconceptions in students' self-explanations of specific instructional material, in our case explanations of code examples. This research is a part of our larger effort to automate the assessment of students' freely generated responses, focusing specifically on their self-explanations of code examples during activities related to code comprehension. In this work, we experiment with zero-shot prompting, Supervised Fine-Tuning (SFT), and preference alignment of LLMs to identify gaps in students' self-explanation. With simple prompting, GPT-4 consistently outperformed LLaMA3 and Mistral in identifying gaps and misconceptions, as confirmed by human evaluations. Additionally, our results suggest that fine-tuned large language models are more effective at identifying gaps in students' explanations compared to zero-shot and few-shot prompting techniques. Furthermore, our findings show that the preference optimization approach using Odds Ratio Preference Optimization (ORPO) outperforms SFT in identifying gaps and misconceptions in students' code explanations.",
    "published_date": "2024-12-09T00:00:00",
    "last_revised_date": "2024-12-09T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.10365.pdf",
    "primary_category": "Computers and Society (cs.CY)",
    "categories": [
        "Computers and Society (cs.CY)",
        "Artificial Intelligence (cs.AI)",
        "Software Engineering (cs.SE)"
    ],
    "keywords": null,
    "num_pages": 9,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 2
    },
    "citationCount": 2,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}