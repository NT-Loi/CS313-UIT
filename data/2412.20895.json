{
    "arxiv_id": "2412.20895",
    "title": "Towards Compatible Fine-tuning for Vision-Language Model Updates",
    "authors": [
        {
            "name": "Z Wang",
            "citations_all": 197,
            "citations_recent": 195,
            "h_index_all": 5,
            "h_index_recent": 5,
            "i10_index_all": 4,
            "i10_index_recent": 4
        },
        {
            "name": "J Liang",
            "citations_all": 6609,
            "citations_recent": 6474,
            "h_index_all": 33,
            "h_index_recent": 32,
            "i10_index_all": 59,
            "i10_index_recent": 56
        },
        {
            "name": "L Sheng",
            "citations_all": 220,
            "citations_recent": 220,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 8,
            "i10_index_recent": 8
        },
        {
            "name": "R He",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "Z Wang",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "T Tan",
            "citations_all": 74504,
            "citations_recent": 33500,
            "h_index_all": 124,
            "h_index_recent": 85,
            "i10_index_all": 553,
            "i10_index_recent": 310
        }
    ],
    "abstract": "So far, efficient fine-tuning has become a popular strategy for enhancing the capabilities of foundation models on downstream tasks by learning plug-and-play modules. However, existing methods overlook a crucial issue: if the underlying foundation model is updated, are these plug-and-play modules still effective? In this paper, we first conduct a detailed analysis of various fine-tuning methods on the CLIP in terms of their compatibility with model updates. The study reveals that many high-performing fine-tuning methods fail to be compatible with the upgraded models. To address this, we propose a novel approach, Class-conditioned Context Optimization (ContCoOp), which integrates learnable prompts with class embeddings using an attention layer before inputting them into the text encoder. Consequently, the prompts can dynamically adapt to the changes in embedding space (due to model updates), ensuring continued effectiveness. Extensive experiments over 15 datasets show that our ContCoOp achieves the highest compatibility over the baseline methods, and exhibits robust out-of-distribution generalization.",
    "published_date": "2024-12-30T00:00:00",
    "last_revised_date": "2024-12-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20895.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 14,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}