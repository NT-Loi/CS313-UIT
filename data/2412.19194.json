{
    "arxiv_id": "2412.19194",
    "title": "Provably Efficient Exploration in Reward Machines with Low Regret",
    "authors": [
        {
            "name": "H Bourel",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "A Jonsson",
            "citations_all": 2958,
            "citations_recent": 2064,
            "h_index_all": 31,
            "h_index_recent": 26,
            "i10_index_all": 70,
            "i10_index_recent": 55
        },
        {
            "name": "OA Maillard",
            "citations_all": 3281,
            "citations_recent": 2233,
            "h_index_all": 30,
            "h_index_recent": 26,
            "i10_index_all": 61,
            "i10_index_recent": 54
        },
        {
            "name": "C Ma",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "MS Talebi",
            "citations_all": 1179,
            "citations_recent": 748,
            "h_index_all": 15,
            "h_index_recent": 12,
            "i10_index_all": 27,
            "i10_index_recent": 13
        }
    ],
    "abstract": "We study reinforcement learning (RL) for decision processes with non-Markovian reward, in which high-level knowledge of the task in the form of reward machines is available to the learner. We consider probabilistic reward machines with initially unknown dynamics, and investigate RL under the average-reward criterion, where the learning performance is assessed through the notion of regret. Our main algorithmic contribution is a model-based RL algorithm for decision processes involving probabilistic reward machines that is capable of exploiting the structure induced by such machines. We further derive high-probability and non-asymptotic bounds on its regret and demonstrate the gain in terms of regret over existing algorithms that could be applied, but obliviously to the structure. We also present a regret lower bound for the studied setting. To the best of our knowledge, the proposed algorithm constitutes the first attempt to tailor and analyze regret specifically for RL with probabilistic reward machines.",
    "published_date": "2024-12-26T00:00:00",
    "last_revised_date": "2024-12-26T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.19194.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
    ],
    "keywords": null,
    "num_pages": 35,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2024": 1
    },
    "citationCount": 1,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}