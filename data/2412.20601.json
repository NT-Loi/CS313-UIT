{
    "arxiv_id": "2412.20601",
    "title": "MATEY: multiscale adaptive foundation models for spatiotemporal physical systems",
    "authors": [
        {
            "name": "P Zhang",
            "citations_all": 401,
            "citations_recent": 317,
            "h_index_all": 14,
            "h_index_recent": 11,
            "i10_index_all": 18,
            "i10_index_recent": 14
        },
        {
            "name": "MP Laiu",
            "citations_all": 265,
            "citations_recent": 248,
            "h_index_all": 10,
            "h_index_recent": 10,
            "i10_index_all": 11,
            "i10_index_recent": 10
        },
        {
            "name": "M Norman",
            "citations_all": 948,
            "citations_recent": 768,
            "h_index_all": 16,
            "h_index_recent": 14,
            "i10_index_all": 24,
            "i10_index_recent": 17
        },
        {
            "name": "D Stefanski",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "J Gounley",
            "citations_all": 2709,
            "citations_recent": 2541,
            "h_index_all": 22,
            "h_index_recent": 20,
            "i10_index_all": 35,
            "i10_index_recent": 33
        }
    ],
    "abstract": "Accurate representation of the multiscale features in spatiotemporal physical systems using vision transformer (ViT) architectures requires extremely long, computationally prohibitive token sequences. To address this issue, we propose two adaptive tokenization schemes that dynamically adjust patch sizes based on local features: one ensures convergent behavior to uniform patch refinement, while the other offers better computational efficiency. Moreover, we present a set of spatiotemporal attention schemes, where the temporal or axial spatial dimensions are decoupled, and evaluate their computational and data efficiencies. We assess the performance of the proposed multiscale adaptive model, MATEY, in a sequence of experiments. The results show that adaptive tokenization schemes achieve improved accuracy without significantly increasing the length of the token sequence. Compared to a full spatiotemporal attention scheme or a scheme that decouples only the temporal dimension, we find that fully decoupled axial attention is less efficient and expressive, requiring more training time and model weights to achieve the same accuracy. Finally, we demonstrate in two fine-tuning tasks featuring different physics that models pretrained on PDEBench data outperform the ones trained from scratch, especially in the low data regime with frozen attention.",
    "published_date": "2024-12-29T00:00:00",
    "last_revised_date": "2024-12-29T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20601.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "keywords": null,
    "num_pages": 18,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 2
    },
    "citationCount": 2,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 33,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 92,
            "citationCount": 2,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}