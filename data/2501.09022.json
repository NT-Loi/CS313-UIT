{
    "arxiv_id": "2501.09022",
    "title": "Generative Models with ELBOs Converging to Entropy Sums",
    "authors": [
        {
            "name": "J Warnken",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "D Velychko",
            "citations_all": 167,
            "citations_recent": 99,
            "h_index_all": 7,
            "h_index_recent": 5,
            "i10_index_all": 5,
            "i10_index_recent": 4
        },
        {
            "name": "S Damm",
            "citations_all": 211,
            "citations_recent": 210,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 3,
            "i10_index_recent": 3
        },
        {
            "name": "A Fischer",
            "citations_all": 9674,
            "citations_recent": 7790,
            "h_index_all": 31,
            "h_index_recent": 29,
            "i10_index_all": 53,
            "i10_index_recent": 52
        },
        {
            "name": "J LÃ¼cke",
            "citations_all": 1759,
            "citations_recent": 813,
            "h_index_all": 23,
            "h_index_recent": 13,
            "i10_index_all": 44,
            "i10_index_recent": 22
        }
    ],
    "abstract": "The evidence lower bound (ELBO) is one of the most central objectives for probabilistic unsupervised learning. For the ELBOs of several generative models and model classes, we here prove convergence to entropy sums. As one result, we provide a list of generative models for which entropy convergence has been shown, so far, along with the corresponding expressions for entropy sums. Our considerations include very prominent generative models such as probabilistic PCA, sigmoid belief nets or Gaussian mixture models. However, we treat more models and entire model classes such as general mixtures of exponential family distributions. Our main contributions are the proofs for the individual models. For each given model we show that the conditions stated in Theorem 1 or Theorem 2 of [arXiv:2209.03077] are fulfilled such that by virtue of the theorems the given model's ELBO is equal to an entropy sum at all stationary points. The equality of the ELBO at stationary points applies under realistic conditions: for finite numbers of data points, for model/data mismatches, at any stationary point including saddle points etc, and it applies for any well behaved family of variational distributions.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.09022.pdf",
    "primary_category": "Machine Learning (stat.ML)",
    "categories": [
        "Machine Learning (stat.ML)",
        "Information Theory (cs.IT)",
        "Machine Learning (cs.LG)",
        "Probability (math.PR)",
        "Statistics Theory (math.ST)"
    ],
    "keywords": null,
    "num_pages": 16,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}