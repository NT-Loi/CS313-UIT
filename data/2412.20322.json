{
    "arxiv_id": "2412.20322",
    "title": "GreenLLM: Disaggregating Large Language Model Serving on Heterogeneous GPUs for Lower Carbon Emissions",
    "authors": [
        {
            "name": "T Shi",
            "citations_all": 69,
            "citations_recent": 69,
            "h_index_all": 4,
            "h_index_recent": 4,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "Y Wu",
            "citations_all": 31,
            "citations_recent": 31,
            "h_index_all": 3,
            "h_index_recent": 3,
            "i10_index_all": 1,
            "i10_index_recent": 1
        },
        {
            "name": "S Liu",
            "citations_all": 1108,
            "citations_recent": 986,
            "h_index_all": 15,
            "h_index_recent": 15,
            "i10_index_all": 20,
            "i10_index_recent": 19
        },
        {
            "name": "Y Ding",
            "citations_all": 527,
            "citations_recent": 471,
            "h_index_all": 12,
            "h_index_recent": 12,
            "i10_index_all": 12,
            "i10_index_recent": 12
        }
    ],
    "abstract": "LLMs have been widely adopted across many real-world applications. However, their widespread use comes with significant environmental costs due to their high computational intensity and resource demands. Specifically, this has driven the development of new generations of high-performing GPUs, exacerbating the problem of electronic waste and accelerating the premature disposal of devices. To address this problem, this paper focuses on reducing the carbon emissions of LLM serving by reusing older, low-performing GPUs. We present GreenLLM, an SLO-aware LLM serving framework designed to minimize carbon emissions by reusing older GPUs. GreenLLM builds on two identified use cases that disaggregate specific computations onto older GPUs, reducing carbon emissions while meeting performance goals. To deepen our understanding of the potential carbon savings from disaggregation, we also provide a theoretical analysis of its relationship with carbon intensity and GPU lifetime. Our evaluations show that GreenLLM reduces carbon emissions by up to 40.6% compared to running standard LLM serving on new GPU only, meeting latency SLOs for over 90% of requests across various applications, latency requirements, carbon intensities, and GPU lifetimes.",
    "published_date": "2024-12-29T00:00:00",
    "last_revised_date": "2024-12-29T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20322.pdf",
    "primary_category": "Hardware Architecture (cs.AR)",
    "categories": [
        "Hardware Architecture (cs.AR)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "keywords": null,
    "num_pages": 15,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 6
    },
    "citationCount": 6,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 44,
            "citationCount": 1,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 31,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 35,
            "citationCount": 3,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 55,
            "citationCount": 10,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 57,
            "citationCount": 7,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 52,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}