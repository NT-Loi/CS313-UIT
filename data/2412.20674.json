{
    "arxiv_id": "2412.20674",
    "title": "Blockchain-Empowered Cyber-Secure Federated Learning for Trustworthy Edge Computing",
    "authors": [
        {
            "name": "E Moore",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "A Imteaj",
            "citations_all": 2207,
            "citations_recent": 2078,
            "h_index_all": 20,
            "h_index_recent": 19,
            "i10_index_all": 32,
            "i10_index_recent": 31
        },
        {
            "name": "MZ Hossain",
            "citations_all": 114,
            "citations_recent": 114,
            "h_index_all": 8,
            "h_index_recent": 8,
            "i10_index_all": 5,
            "i10_index_recent": 5
        },
        {
            "name": "S Rezapour",
            "citations_all": 4241,
            "citations_recent": 2325,
            "h_index_all": 26,
            "h_index_recent": 20,
            "i10_index_all": 34,
            "i10_index_recent": 31
        },
        {
            "name": "MH Amini",
            "citations_all": 7870,
            "citations_recent": 5752,
            "h_index_all": 44,
            "h_index_recent": 35,
            "i10_index_all": 100,
            "i10_index_recent": 89
        }
    ],
    "abstract": "Federated Learning (FL) is a privacy-preserving distributed machine learning scheme, where each participant data remains on the participating devices and only the local model generated utilizing the local computational power is transmitted throughout the database. However, the distributed computational nature of FL creates the necessity to develop a mechanism that can remotely trigger any network agents, track their activities, and prevent threats to the overall process posed by malicious participants. Particularly, the FL paradigm may become vulnerable due to an active attack from the network participants, called a poisonous attack. In such an attack, the malicious participant acts as a benign agent capable of affecting the global model quality by uploading an obfuscated poisoned local model update to the server. This paper presents a cross-device FL model that ensures trustworthiness, fairness, and authenticity in the underlying FL training process. We leverage trustworthiness by constructing a reputation-based trust model based on contributions of agents toward model convergence. We ensure fairness by identifying and removing malicious agents from the training process through an outlier detection technique. Further, we establish authenticity by generating a token for each participating device through a distributed sensing mechanism and storing that unique token in a blockchain smart contract. Further, we insert the trust scores of all agents into a blockchain and validate their reputations using various consensus mechanisms that consider the computational task.",
    "published_date": "2024-12-30T00:00:00",
    "last_revised_date": "2024-12-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.20674.pdf",
    "primary_category": "Distributed, Parallel, and Cluster Computing (cs.DC)",
    "categories": [
        "Distributed, Parallel, and Cluster Computing (cs.DC)",
        "Cryptography and Security (cs.CR)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 9,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 9,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}