{
    "arxiv_id": "2501.00303",
    "title": "SAM-Aware Graph Prompt Reasoning Network for Cross-Domain Few-Shot Segmentation",
    "authors": [
        {
            "name": "SF Peng",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "G Sun",
            "citations_all": 8738,
            "citations_recent": 8680,
            "h_index_all": 21,
            "h_index_recent": 21,
            "i10_index_all": 26,
            "i10_index_recent": 26
        },
        {
            "name": "Y Li",
            "citations_all": 2810,
            "citations_recent": 2744,
            "h_index_all": 19,
            "h_index_recent": 19,
            "i10_index_all": 24,
            "i10_index_recent": 24
        },
        {
            "name": "H Wang",
            "citations_all": 1082,
            "citations_recent": 923,
            "h_index_all": 11,
            "h_index_recent": 11,
            "i10_index_all": 11,
            "i10_index_recent": 11
        },
        {
            "name": "GS Xie",
            "citations_all": 4747,
            "citations_recent": 4363,
            "h_index_all": 33,
            "h_index_recent": 31,
            "i10_index_all": 60,
            "i10_index_recent": 57
        }
    ],
    "abstract": "The primary challenge of cross-domain few-shot segmentation (CD-FSS) is the domain disparity between the training and inference phases, which can exist in either the input data or the target classes. Previous models struggle to learn feature representations that generalize to various unknown domains from limited training domain samples. In contrast, the large-scale visual model SAM, pre-trained on tens of millions of images from various domains and classes, possesses excellent generalizability. In this work, we propose a SAM-aware graph prompt reasoning network (GPRN) that fully leverages SAM to guide CD-FSS feature representation learning and improve prediction accuracy. Specifically, we propose a SAM-aware prompt initialization module (SPI) to transform the masks generated by SAM into visual prompts enriched with high-level semantic information. Since SAM tends to divide an object into many sub-regions, this may lead to visual prompts representing the same semantic object having inconsistent or fragmented features. We further propose a graph prompt reasoning (GPR) module that constructs a graph among visual prompts to reason about their interrelationships and enable each visual prompt to aggregate information from similar prompts, thus achieving global semantic consistency. Subsequently, each visual prompt embeds its semantic information into the corresponding mask region to assist in feature representation learning. To refine the segmentation mask during testing, we also design a non-parameter adaptive point selection module (APS) to select representative point prompts from query predictions and feed them back to SAM to refine inaccurate segmentation results. Experiments on four standard CD-FSS datasets demonstrate that our method establishes new state-of-the-art results. Code: this https URL.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00303.pdf",
    "primary_category": "Computer Vision and Pattern Recognition (cs.CV)",
    "categories": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 16,
    "github_stars": 30,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 3,
    "venue": {
        "name": "AAAI Conference on Artificial Intelligence",
        "type": "conference",
        "ranking": "A"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 55,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}