{
    "arxiv_id": "2501.00265",
    "title": "Outlier-Robust Training of Machine Learning Models",
    "authors": [
        {
            "name": "R Talak",
            "citations_all": 1948,
            "citations_recent": 1730,
            "h_index_all": 18,
            "h_index_recent": 18,
            "i10_index_all": 24,
            "i10_index_recent": 22
        },
        {
            "name": "C Georgiou",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "J Shi",
            "citations_all": 2132,
            "citations_recent": 2128,
            "h_index_all": 11,
            "h_index_recent": 11,
            "i10_index_all": 13,
            "i10_index_recent": 13
        },
        {
            "name": "L Carlone",
            "citations_all": 20477,
            "citations_recent": 16997,
            "h_index_all": 62,
            "h_index_recent": 53,
            "i10_index_all": 137,
            "i10_index_recent": 124
        }
    ],
    "abstract": "Robust training of machine learning models in the presence of outliers has garnered attention across various domains. The use of robust losses is a popular approach and is known to mitigate the impact of outliers. We bring to light two literatures that have diverged in their ways of designing robust losses: one using M-estimation, which is popular in robotics and computer vision, and another using a risk-minimization framework, which is popular in deep learning. We first show that a simple modification of the Black-Rangarajan duality provides a unifying view. The modified duality brings out a definition of a robust loss kernel $\\sigma$ that is satisfied by robust losses in both the literatures. Secondly, using the modified duality, we propose an Adaptive Alternation Algorithm (AAA) for training machine learning models with outliers. The algorithm iteratively trains the model by using a weighted version of the non-robust loss, while updating the weights at each iteration. The algorithm is augmented with a novel parameter update rule by interpreting the weights as inlier probabilities, and obviates the need for complex parameter tuning. Thirdly, we investigate convergence of the adaptive alternation algorithm to outlier-free optima. Considering arbitrary outliers (i.e., with no distributional assumption on the outliers), we show that the use of robust loss kernels {\\sigma} increases the region of convergence. We experimentally show the efficacy of our algorithm on regression, classification, and neural scene reconstruction problems. We release our implementation code: this https URL.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00265.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "keywords": null,
    "num_pages": 31,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 1
    },
    "citationCount": 1,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}