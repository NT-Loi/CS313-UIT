{
    "arxiv_id": "2412.18729",
    "title": "Optimizing Large Language Models with an Enhanced LoRA Fine-Tuning Algorithm for Efficiency and Robustness in NLP Tasks",
    "authors": [
        {
            "name": "J Hu",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "X Liao",
            "citations_all": 148,
            "citations_recent": 148,
            "h_index_all": 9,
            "h_index_recent": 9,
            "i10_index_all": 9,
            "i10_index_recent": 9
        },
        {
            "name": "J Gao",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "Z Qi",
            "citations_all": 241,
            "citations_recent": 241,
            "h_index_all": 13,
            "h_index_recent": 13,
            "i10_index_all": 14,
            "i10_index_recent": 14
        },
        {
            "name": "H Zheng",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "C Wang",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        }
    ],
    "abstract": "This study proposes a large language model optimization method based on the improved LoRA fine-tuning algorithm, aiming to improve the accuracy and computational efficiency of the model in natural language processing tasks. We fine-tune the large language model through a low-rank adaptation strategy, which significantly reduces the consumption of computing resources while maintaining the powerful capabilities of the pre-trained model. The experiment uses the QQP task as the evaluation scenario. The results show that the improved LoRA algorithm shows significant improvements in accuracy, F1 score, and MCC compared with traditional models such as BERT, Roberta, T5, and GPT-4. In particular, in terms of F1 score and MCC, our model shows stronger robustness and discrimination ability, which proves the potential of the improved LoRA algorithm in fine-tuning large-scale pre-trained models. In addition, this paper also discusses the application prospects of the improved LoRA algorithm in other natural language processing tasks, emphasizing its advantages in multi-task learning and scenarios with limited computing resources. Future research can further optimize the LoRA fine-tuning strategy and expand its application in larger-scale pre-trained models to improve the generalization ability and task adaptability of the model.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.18729.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Machine Learning (cs.LG)"
    ],
    "keywords": null,
    "num_pages": 5,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2024": 2,
        "2025": 14
    },
    "citationCount": 17,
    "venue": {
        "name": null,
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 18,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 21,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 26,
            "citationCount": 7,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 17,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 18,
            "citationCount": 9,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 22,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 31,
            "citationCount": 17,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 14,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 11,
            "citationCount": 20,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 21,
            "citationCount": 47,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 12,
            "citationCount": 7,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 11,
            "citationCount": 60,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 45,
            "citationCount": 24,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 31,
            "citationCount": 17,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 16,
            "citationCount": 3,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 69,
            "citationCount": 83,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 30,
            "citationCount": 72,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 125,
            "citationCount": 217,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 42,
            "citationCount": 175,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 46,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 25,
            "citationCount": 29,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 17898,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 98,
            "citationCount": 288,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 33,
            "citationCount": 26,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 41,
            "citationCount": 186,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 1,
    "embedding": null
}