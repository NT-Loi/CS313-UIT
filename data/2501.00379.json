{
    "arxiv_id": "2501.00379",
    "title": "Federated Dropout: Convergence Analysis and Resource Allocation",
    "authors": [
        {
            "name": "S Xie",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "D Wen",
            "citations_all": 1892,
            "citations_recent": 1797,
            "h_index_all": 18,
            "h_index_recent": 17,
            "i10_index_all": 26,
            "i10_index_recent": 24
        },
        {
            "name": "X Liu",
            "citations_all": 1170,
            "citations_recent": 1055,
            "h_index_all": 13,
            "h_index_recent": 13,
            "i10_index_all": 17,
            "i10_index_recent": 15
        },
        {
            "name": "C You",
            "citations_all": 20446,
            "citations_recent": 18298,
            "h_index_all": 46,
            "h_index_recent": 45,
            "i10_index_all": 85,
            "i10_index_recent": 85
        },
        {
            "name": "T Ratnarajah",
            "citations_all": 10210,
            "citations_recent": 4758,
            "h_index_all": 51,
            "h_index_recent": 35,
            "i10_index_all": 228,
            "i10_index_recent": 127
        },
        {
            "name": "K Huang",
            "citations_all": 28373,
            "citations_recent": 20023,
            "h_index_all": 72,
            "h_index_recent": 58,
            "i10_index_all": 188,
            "i10_index_recent": 153
        }
    ],
    "abstract": "Federated Dropout is an efficient technique to overcome both communication and computation bottlenecks for deploying federated learning at the network edge. In each training round, an edge device only needs to update and transmit a sub-model, which is generated by the typical method of dropout in deep learning, and thus effectively reduces the per-round latency. \\textcolor{blue}{However, the theoretical convergence analysis for Federated Dropout is still lacking in the literature, particularly regarding the quantitative influence of dropout rate on convergence}. To address this issue, by using the Taylor expansion method, we mathematically show that the gradient variance increases with a scaling factor of $\\gamma/(1-\\gamma)$, with $\\gamma \\in [0, \\theta)$ denoting the dropout rate and $\\theta$ being the maximum dropout rate ensuring the loss function reduction. Based on the above approximation, we provide the convergence analysis for Federated Dropout. Specifically, it is shown that a larger dropout rate of each device leads to a slower convergence rate. This provides a theoretical foundation for reducing the convergence latency by making a tradeoff between the per-round latency and the overall rounds till convergence. Moreover, a low-complexity algorithm is proposed to jointly optimize the dropout rate and the bandwidth allocation for minimizing the loss function in all rounds under a given per-round latency and limited network resources. Finally, numerical results are provided to verify the effectiveness of the proposed algorithm.",
    "published_date": "2024-12-31T00:00:00",
    "last_revised_date": "2024-12-31T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00379.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Information Theory (cs.IT)"
    ],
    "keywords": null,
    "num_pages": 13,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2025": 4
    },
    "citationCount": 4,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 41,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 50,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 46,
            "citationCount": 0,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}