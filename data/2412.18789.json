{
    "arxiv_id": "2412.18789",
    "title": "On Improved Regret Bounds In Bayesian Optimization with Gaussian Noise",
    "authors": [
        {
            "name": "J Wang",
            "citations_all": null,
            "citations_recent": null,
            "h_index_all": null,
            "h_index_recent": null,
            "i10_index_all": null,
            "i10_index_recent": null
        },
        {
            "name": "H Wang",
            "citations_all": 121,
            "citations_recent": 121,
            "h_index_all": 6,
            "h_index_recent": 6,
            "i10_index_all": 2,
            "i10_index_recent": 2
        },
        {
            "name": "CG Petra",
            "citations_all": 1343,
            "citations_recent": 740,
            "h_index_all": 18,
            "h_index_recent": 16,
            "i10_index_all": 26,
            "i10_index_recent": 20
        },
        {
            "name": "NY Chiang",
            "citations_all": 395,
            "citations_recent": 257,
            "h_index_all": 11,
            "h_index_recent": 10,
            "i10_index_all": 11,
            "i10_index_recent": 11
        }
    ],
    "abstract": "Bayesian optimization (BO) with Gaussian process (GP) surrogate models is a powerful black-box optimization method. Acquisition functions are a critical part of a BO algorithm as they determine how the new samples are selected. Some of the most widely used acquisition functions include upper confidence bound (UCB) and Thompson sampling (TS). The convergence analysis of BO algorithms has focused on the cumulative regret under both the Bayesian and frequentist settings for the objective. In this paper, we establish new pointwise bounds on the prediction error of GP under the frequentist setting with Gaussian noise. Consequently, we prove improved convergence rates of cumulative regret bound for both GP-UCB and GP-TS. Of note, the new prediction error bound under Gaussian noise can be applied to general BO algorithms and convergence analysis, e.g., the asymptotic convergence of expected improvement (EI) with noise.",
    "published_date": "2024-12-25T00:00:00",
    "last_revised_date": "2024-12-25T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2412.18789.pdf",
    "primary_category": "Machine Learning (cs.LG)",
    "categories": [
        "Machine Learning (cs.LG)",
        "Machine Learning (stat.ML)"
    ],
    "keywords": [
        "Bayesian optimization",
        "Gaussian process",
        "upper confidence bound",
        "Thompson sampling",
        "Gaussian noise",
        "cumulative regret"
    ],
    "num_pages": 19,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 0,
    "venue": {
        "name": "arXiv.org",
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}