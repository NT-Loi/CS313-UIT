{
    "arxiv_id": "2501.00154",
    "title": "Probabilistic Explanations for Linear Models",
    "authors": [
        {
            "name": "B Subercaseaux",
            "citations_all": 406,
            "citations_recent": 405,
            "h_index_all": 10,
            "h_index_recent": 10,
            "i10_index_all": 10,
            "i10_index_recent": 10
        },
        {
            "name": "M Arenas",
            "citations_all": 13129,
            "citations_recent": 4247,
            "h_index_all": 54,
            "h_index_recent": 29,
            "i10_index_all": 109,
            "i10_index_recent": 59
        },
        {
            "name": "KS Meel",
            "citations_all": 4113,
            "citations_recent": 3288,
            "h_index_all": 33,
            "h_index_recent": 33,
            "i10_index_all": 73,
            "i10_index_recent": 70
        }
    ],
    "abstract": "Formal XAI is an emerging field that focuses on providing explanations with mathematical guarantees for the decisions made by machine learning models. A significant amount of work in this area is centered on the computation of \"sufficient reasons\". Given a model $M$ and an input instance $\\vec{x}$, a sufficient reason for the decision $M(\\vec{x})$ is a subset $S$ of the features of $\\vec{x}$ such that for any instance $\\vec{z}$ that has the same values as $\\vec{x}$ for every feature in $S$, it holds that $M(\\vec{x}) = M(\\vec{z})$. Intuitively, this means that the features in $S$ are sufficient to fully justify the classification of $\\vec{x}$ by $M$. For sufficient reasons to be useful in practice, they should be as small as possible, and a natural way to reduce the size of sufficient reasons is to consider a probabilistic relaxation; the probability of $M(\\vec{x}) = M(\\vec{z})$ must be at least some value $\\delta \\in (0,1]$, for a random instance $\\vec{z}$ that coincides with $\\vec{x}$ on the features in $S$. Computing small $\\delta$-sufficient reasons ($\\delta$-SRs) is known to be a theoretically hard problem; even over decision trees--traditionally deemed simple and interpretable models--strong inapproximability results make the efficient computation of small $\\delta$-SRs unlikely. We propose the notion of $(\\delta, \\epsilon)$-SR, a simple relaxation of $\\delta$-SRs, and show that this kind of explanation can be computed efficiently over linear models.",
    "published_date": "2024-12-30T00:00:00",
    "last_revised_date": "2024-12-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2501.00154.pdf",
    "primary_category": "Artificial Intelligence (cs.AI)",
    "categories": [
        "Artificial Intelligence (cs.AI)",
        "Computational Complexity (cs.CC)"
    ],
    "keywords": null,
    "num_pages": 25,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {},
    "citationCount": 5,
    "venue": {
        "name": null,
        "type": null,
        "ranking": null
    },
    "citations": [],
    "referenceCount": 0,
    "references": [],
    "influentialCitationCount": 0,
    "embedding": null
}