{
    "arxiv_id": "2112.11441",
    "title": "NLP Techniques for Water Quality Analysis in Social Media Content",
    "authors": [
        {
            "name": "MA Ayub",
            "citations_all": 64,
            "citations_recent": 64,
            "h_index_all": 6,
            "h_index_recent": 6,
            "i10_index_all": 3,
            "i10_index_recent": 3
        },
        {
            "name": "K Ahmad",
            "citations_all": 82,
            "citations_recent": 82,
            "h_index_all": 6,
            "h_index_recent": 6,
            "i10_index_all": 4,
            "i10_index_recent": 4
        },
        {
            "name": "K Ahmad",
            "citations_all": 3838,
            "citations_recent": 3645,
            "h_index_all": 28,
            "h_index_recent": 28,
            "i10_index_all": 50,
            "i10_index_recent": 47
        },
        {
            "name": "N Ahmad",
            "citations_all": 2717,
            "citations_recent": 2171,
            "h_index_all": 18,
            "h_index_recent": 14,
            "i10_index_all": 38,
            "i10_index_recent": 28
        },
        {
            "name": "A Al-Fuqaha",
            "citations_all": 32079,
            "citations_recent": 25412,
            "h_index_all": 55,
            "h_index_recent": 53,
            "i10_index_all": 164,
            "i10_index_recent": 133
        }
    ],
    "abstract": "This paper presents our contributions to the MediaEval 2021 task namely \"WaterMM: Water Quality in Social Multimedia\". The task aims at analyzing social media posts relevant to water quality with particular focus on the aspects like watercolor, smell, taste, and related illnesses. To this aim, a multimodal dataset containing both textual and visual information along with meta-data is provided. Considering the quality and quantity of available content, we mainly focus on textual information by employing three different models individually and jointly in a late-fusion manner. These models include (i) Bidirectional Encoder Representations from Transformers (BERT), (ii) Robustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and a (iii) custom Long short-term memory (LSTM) model obtaining an overall F1-score of 0.794, 0.717, 0.663 on the official test set, respectively. In the fusion scheme, all the models are treated equally and no significant improvement is observed in the performance over the best performing individual model.",
    "published_date": "2021-11-30T00:00:00",
    "last_revised_date": "2021-11-30T00:00:00",
    "num_revisions": 0,
    "pdf_url": "https://arxiv.org/pdf/2112.11441.pdf",
    "primary_category": "Computation and Language (cs.CL)",
    "categories": [
        "Computation and Language (cs.CL)",
        "Social and Information Networks (cs.SI)"
    ],
    "keywords": null,
    "num_pages": 3,
    "github_stars": null,
    "upvote": 0,
    "citing_models": 0,
    "citing_datasets": 0,
    "citing_spaces": 0,
    "citing_collections": 0,
    "citations_by_year": {
        "2022": 3,
        "2023": 2,
        "2024": 1
    },
    "citationCount": 6,
    "venue": {
        "name": "MediaEval Benchmarking Initiative for Multimedia Evaluation",
        "type": "conference"
    },
    "citations": [
        {
            "arxiv_id": null,
            "referenceCount": 32,
            "citationCount": 0,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 98,
            "citationCount": 6,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 23,
            "citationCount": 7,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 22,
            "citationCount": 1,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 36,
            "citationCount": 11,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 29,
            "citationCount": 5,
            "influentialCitationCount": null
        }
    ],
    "referenceCount": 9,
    "references": [
        {
            "arxiv_id": null,
            "referenceCount": 100,
            "citationCount": 274,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 13,
            "citationCount": 46,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 16,
            "citationCount": 355,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 0,
            "citationCount": 12599,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 68,
            "citationCount": 27222,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 152,
            "citationCount": 120,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 8,
            "citationCount": 8,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": 63,
            "citationCount": 105570,
            "influentialCitationCount": null
        },
        {
            "arxiv_id": null,
            "referenceCount": null,
            "citationCount": null,
            "influentialCitationCount": null
        }
    ],
    "influentialCitationCount": 0,
    "embedding": null
}